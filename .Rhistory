setwd("~/GitHub/SCZ-CNV")
#Setting working directory
rm(list=ls(env=.GlobalEnv))
#Cleaning up workspace
packages <- c("dplyr","httr","stringr","gprofiler2","rvest","tidyr")
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
if(!"rWikiPathways" %in% installed.packages()){
if (!requireNamespace("BiocManager", quietly=TRUE))
install.packages("BiocManager")
BiocManager::install("rWikiPathways")
}
if(!"RCy3" %in% installed.packages()){
if (!requireNamespace("BiocManager", quietly=TRUE))
install.packages("BiocManager")
BiocManager::install("RCy3")
}
if(!"BridgeDbR" %in% installed.packages()){
if (!requireNamespace("BiocManager", quietly=TRUE))
install.packages("BiocManager")
BiocManager::install("BridgeDbR")
}
#Checking if required packages are installed and installing if not
#Different structure for rWikiPathways and RCy3 packages as these are not installed directly but via the BiocManager package
invisible(lapply(c(packages,"rWikiPathways","RCy3","BridgeDbR"), require, character.only = TRUE))
#Loading libraries
sysdatetime <- Sys.time()
datetime <- format(sysdatetime, format = "%Y-%m-%d_%Hh%M")
dir.create("Outputs")
dir.create(sprintf("Outputs/Session-%s",datetime))
dir.create(sprintf("Outputs/Session-%s/Networks",datetime))
dir.create(sprintf("Outputs/Session-%s/Other",datetime))
#Creating directories for outputs generated by this script to be saved in; new "Session" folder created each time the script is ran (contains generated networks, metadata, and sessionInfo)
nw_savepath <- sprintf("%1$s/Outputs/Session-%2$s/Networks/",getwd(),datetime)
other_savepath <- sprintf("%1$s/Outputs/Session-%2$s/Other/",getwd(),datetime)
dir.create(paste0(other_savepath,"WikiPathways"))
dir.create(paste0(other_savepath,"DisGeNET"))
dir.create(paste0(other_savepath,"AOP-Wiki"))
dir.create(paste0(other_savepath,"Clustering"))
dir.create(paste0(other_savepath,"CyTargetLinker"))
file.create(sprintf("Outputs/Session-%s/metadata.txt",datetime))
#Creating a new metadata file with the current date and time as suffix for easier organisation
#Such a metadata file should be generated every time this script is ran to record parameters and versions of functions or databases, including the time avoids files being overwritten if the script is run multiple times a day (can even include seconds if script is ran multiple times per minute)
metadata.add <- function(info) {
write(sapply(info, as.character), sprintf("Outputs/Session-%s/metadata.txt",datetime),append=TRUE, sep = "\n")
}
metadata.add(sysdatetime)
metadata.add(Sys.timezone())
metadata.add("")
#Adding the timezone, date, and time to the metadata
invisible(file.create(sprintf("Outputs/Session-%s/sessioninfo-%s.txt",datetime,datetime)))
writeLines(capture.output(sessionInfo()),sprintf("Outputs/Session-%s/sessioninfo-%s.txt",datetime,datetime))
#Generating and adding a sessionInfo file to the current session output folder
execution_times <- list()
start_section <- function(section_name) {
execution_times[[section_name]] <<- Sys.time()
}
end_section <- function(section_name) {
end_time <- Sys.time()
start_time <- execution_times[[section_name]]
execution_time <- end_time - start_time
# Convert elapsed time to seconds
execution_time_seconds <- as.numeric(execution_time, units = "secs")
# Convert seconds to a human-readable format with two decimal places
time_string <- format(round(execution_time_seconds, 2), nsmall = 2)
# Add "s" for seconds
time_string <- paste(time_string, "s", sep = "")
measurements <- paste(section_name, "\t", time_string, "\n", sep = "")
cat(measurements, file = sprintf("Outputs/Session-%s/execution-times.txt",datetime), append = TRUE)
# Remove start time from the list
execution_times[[section_name]] <- NULL
}
cytoscapePing()
cytoscapeVersionInfo()
#Checking if Cytoscape is running and version info
metadata.add(capture.output(cytoscapeVersionInfo()))
checkinstall.app <- function(app) {
status_string <- getAppStatus(app)
#Getting install status of app
words <- strsplit(status_string, " ")[[1]]
last_word <- tail(words, 1)
#getAppStatus returns a character string instead of a logical value, so the last word (usually either "Installed" or "Uninstalled") from the output is checked
if (last_word == "Installed") {
print(sprintf("App %s is already installed.",app))
} else {
installApp(app)
print(sprintf("Installed app %s.",app))
}
}
#Function to check whether required Cytoscape apps are installed and installing them if not
applist <- c("stringApp","clusterMaker2","yFiles Layout Algorithms")
#WikiPathways v.3.3.10
#DisGeNET-app v.7.3.0
#CyTargetLinker v. 4.1.0
#stringApp v. 2.0.2
#BridgeDb v.1.2.0
#clusterMaker2 v.2.3.4
lapply(applist,checkinstall.app)
#Checking and installing (if required) necessary Cytoscape apps
lapply(applist,getAppInformation)
metadata.add("Required Cytoscape apps and versions:")
invisible(metadata.add(print(lapply(applist,getAppInformation))))
metadata.add("")
dir.create(paste0(getwd(),"/BridgeDb"))
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "BridgeDb mapping file not detected. Download BridgeDb mapping file for Homo sapiens (800.4 MB)? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=paste0(getwd(),"/BridgeDb"))
message("BridgeDb mapping file downloaded to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
#Downloading the BridgeDb bridge file for Homo sapiens identifiers to the repo
#The directory is added to .gitignore to avoid uploading it to GitHub
#This is a relatively large (800MB) file - downloading it once is sufficient
loadDatabase(bridgedb_dir)
#Loading the database
metadata.add("BridgeDb")
metadata.add("Homo sapiens bridge version 108")
metadata.add("")
# FUNCTION DICTIONARY-------------------------------------------------------------------------------------------------------------------
.defaultBaseUrl <- 'http://127.0.0.1:1234/v1'
#Defining the default base URL found in the RCy3 source as R object for altmergeNetworks
altmergeNetworks <- function(               sources = NULL,
title = NULL,
operation = "union",
nodeKeys = NULL,
nodeMergeMap = NULL,
nodesOnly = FALSE,
edgeKeys = NULL,
edgeMergeMap = NULL,
networkMergeMap = NULL,
inNetworkMerge = TRUE,
base.url = .defaultBaseUrl) {
cmd.string <- 'network merge' # a good start
# sources must be suppled
if(is.null(sources)) {
message("Missing sources!")
return(NULL)
} else {
sources.str <- paste(sources, collapse = ",")
cmd.string <- paste0(cmd.string,' sources="',sources.str,'"')
}
# defaults
cmd.string <- paste0(cmd.string,' operation=',operation)
cmd.string <- paste0(cmd.string,' nodesOnly=',nodesOnly)
cmd.string <- paste0(cmd.string,' inNetworkMerge=',inNetworkMerge)
# optional args
if(!is.null(title))
cmd.string <- paste0(cmd.string,' netName="',title,'"')
if(!is.null(nodeKeys))
cmd.string <- paste0(cmd.string,' nodeKeys="',paste(nodeKeys, collapse = ","),'"')
if(!is.null(edgeKeys))
cmd.string <- paste0(cmd.string,' edgeKeys="',paste(edgeKeys, collapse = ","),'"')
if(!is.null(nodeMergeMap)){
nodeMergeMap.str <- paste(nodeMergeMap, collapse = ",")
nodeMergeMap.str <- gsub("c\\(", "{", nodeMergeMap.str)
nodeMergeMap.str <- gsub("\\)", "}", nodeMergeMap.str)
cmd.string <- paste0(cmd.string,' nodeMergeMap="',nodeMergeMap.str,'"')
}
if(!is.null(edgeMergeMap)){
edgeMergeMap.str <- paste(edgeMergeMap, collapse = ",")
edgeMergeMap.str <- gsub("c\\(", "{", edgeMergeMap.str)
edgeMergeMap.str <- gsub("\\)", "}", edgeMergeMap.str)
cmd.string <- paste0(cmd.string,' edgeMergeMap="',edgeMergeMap.str,'"')
}
if(!is.null(networkMergeMap)){
networkMergeMap.str <- paste(networkMergeMap, collapse = ",")
networkMergeMap.str <- gsub("c\\(", "{", networkMergeMap.str)
networkMergeMap.str <- gsub("\\)", "}", networkMergeMap.str)
cmd.string <- paste0(cmd.string,' networkMergeMap="',networkMergeMap.str,'"')
}
res.data <- commandsPOST(cmd.string, base.url = base.url)
if(!is.null(res.data$SUID))
return(res.data$SUID)
else
return(res.data)
}
#Normally, RCy3's 'mergeNetworks' function would be used to unify imported networks into one supernetwork
#This function does however not work on the latest RCy3 release (v.2.22.1), but does work when running the script on RCy3 v.2.14.2
#RCy3 2.14.2 requires R v.4.1.3, requiring the entire script to run on an old version of R for one function that is used once
#Here, we redefine the function using the source code from RCy3 v.2.14.2 and simply use this alternate function to merge networks
sparqlquery <- function(endpoint,queryfile,output) {
if (tolower(endpoint) %in% c("WikiPathways","wikipathways","wp","WP")) {
file_path <- paste0(getwd(),sprintf("/Data/WikiPathways/%s",queryfile))
#Specifying file path to the .txt file containing the query
base_url <- "https://sparql.wikipathways.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else if (tolower(endpoint) %in% c("AOP-Wiki", "aopwiki","aop-wiki","AOPWiki")) {
file_path <- paste0(getwd(),sprintf("/Data/AOP-Wiki/%s",queryfile))
#Specifying file path to the .txt file containing the query
base_url <- "https://aopwiki.rdf.bigcat-bioinformatics.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else if (tolower(endpoint) %in% c("DisGeNET","DGN","disgenet","Disgenet")) {
file_path <- paste0(getwd(),sprintf("/Data/DisGeNET/%s",queryfile))
#Specifying path to the .txt file containing the query
base_url <- "http://rdf.disgenet.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else
print("Error: Please specify WikiPathways, DisGeNET or AOP-Wiki as endpoint.")
querybody <- paste(readLines(file_path), collapse = "")
#Reading a text file containing a SPARQL query stored in the repo
encoded_query <- URLencode(querybody)
#Encoding the query as URL
full_url <- paste0(base_url, encoded_query, "&format=text/html&timeout=0&signal_void=on")
#Joining the query from the text file and the base URL and adding that output is desired as HTML
html <- read_html(full_url)
#Sending the query to the SPARQL endpoint and extracting as HTML
if(tolower(endpoint) %in% c("DisGeNET","DGN","disgenet","Disgenet")) {
tablebody <- html %>%
html_element("body") %>%
html_element("table") }
#DisGeNET has a slightly different HTML structure than AOP-Wiki and WikiPathways, so navigating to the table is done accordingly
else
tablebody <- html %>%
html_element("body") %>%
html_element("div") %>%
html_element("table")
#Navigating to the table output by the SPARQL query
assign(output, html_table(tablebody), envir=.GlobalEnv)
#Getting the output as tibble
}
#Function to send a SPARQL query defined in a local text file to the endpoint and extract to desired dataframe
#Will not work if query includes comments ('# this is a comment') due to HTML conversion
createNodeSource <- function(source,doi=NULL) {
if (source == "fromDisGeNET") {
networkname <- getNetworkName()
nodetable <- paste0(networkname," default  node")
}
#Networks imported from WikiPathways have a type in the node table designations, as they have 2 spaces between "default" and "node" instead of one
#This check determines which node table name format is to be applied depending on the source (WikiPathways or other)
else {
networkname <- getNetworkName()
nodetable <- paste0(networkname," default node")
}
commandsRun(sprintf("table create column columnName=fromWikiPathways table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromDisGeNET table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromPublication table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=Publication.doi table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromSTRING table=%s type=string",nodetable))
#Creating a new column for each source used for all networks
if ( source == "STRINGnode") {
commandsRun(sprintf('table set values columnName=%1$s handleEquations=false rowList="selected:true" table=%2$s value=1',source,nodetable))
}
else {
commandsRun(sprintf("table set values columnName=%1$s handleEquations=false rowList=all table=%2$s value=1",source,nodetable))
#Filling the new column of the corresponding source with 1 to indicate which source the node is imported from
}
if (!is.null(doi)) {
commandsRun(sprintf("table set values columnName=Publication.doi handleEquations=false rowList=all table=%1$s value=%2$s",nodetable,doi))
#Adding doi for literature used if provided
}
}
#Function to cr
getlinkset <- function(url,dest) {
if (!file.exists(dest)) {
dir.create(dest)
}
file <- "chembl_23_hsa_20180126.zip"
curl_download(url, destfile=file.path(dest,file))
unzip(zipfile=file.path(dest,file),exdir=dest)
}
#Setting up directory creation, file download and unzip for chembl linkset from CyTargetLinker website
linkset_url <- "https://ndownloader.figshare.com/files/21623691?private_link=6cf358aaaaf5adeecce9"
linkset_dir <- paste0(getwd(),"/Data/CyTargetLinker")
if (!file.exists(file.path(linkset_dir, "chembl_23_hsa_20180126.xgmml"))) {
getlinkset(linkset_url,linkset_dir)
print("Linkset downloaded and unzipped.")
file.remove(paste0(getwd(),"/Data/CyTargetLinker/chembl_23_hsa_20180126.zip"))
#Deleting the zip file after extracting the desired xgmml file from it
} else {
print("Linkset already exists.")
}
#Downloading and unzipping if necessary
chembl_path <- paste0(getwd(),"/Data/CyTargetLinker/chembl_23_hsa_20180126.xgmml")
#Defining path to chembl linkset for easy access
commandsRun(sprintf('cytargetlinker extend direction=SOURCES idAttribute=Ensembl  linkSetFiles=%s network=current',chembl_path))
getlinkset <- function(url,dest) {
if (!file.exists(dest)) {
dir.create(dest)
}
file <- "chembl_23_hsa_20180126.zip"
curl_download(url, destfile=file.path(dest,file))
unzip(zipfile=file.path(dest,file),exdir=dest)
}
#Setting up directory creation, file download and unzip for chembl linkset from CyTargetLinker website
linkset_url <- "https://ndownloader.figshare.com/files/21623691?private_link=6cf358aaaaf5adeecce9"
linkset_dir <- paste0(getwd(),"/Data/CyTargetLinker")
if (!file.exists(file.path(linkset_dir, "chembl_23_hsa_20180126.xgmml"))) {
getlinkset(linkset_url,linkset_dir)
print("Linkset downloaded and unzipped.")
file.remove(paste0(getwd(),"/Data/CyTargetLinker/chembl_23_hsa_20180126.zip"))
#Deleting the zip file after extracting the desired xgmml file from it
} else {
print("Linkset already exists.")
}
#Downloading and unzipping if necessary
chembl_path <- paste0(getwd(),"/Data/CyTargetLinker/chembl_23_hsa_20180126.xgmml")
#Defining path to chembl linkset for easy access
commandsRun(sprintf('cytargetlinker extend direction=SOURCES idAttribute=Ensembl  linkSetFiles=%s network=current',chembl_path))
#Extending the pathway-gene-AOP network with chemicals from the linkset
mapped_chembls <- getTableColumns("node","CTL.ChEMBL")
mapped_chembls <- na.omit(mapped_chembls)
#Getting mapped chemicals and removing NA from df for further processing
write.table(mapped_chembls, file=paste0(other_savepath,"CyTargetLinker/mapped_chembls.tsv"),sep="\t",row.names=FALSE,col.names=FALSE,quote=FALSE)
#Writing ChEMBL IDs to file
#ChEMBL IDs are currently mapped to ChEBI IDs using the FixID webservice manually
chembl_chebi <- read.delim(paste0(getwd(),"/Data/CyTargetLinker/chembl-chebi.tsv"),sep="\t")
#Loading ChEMBL-ChEBI mapping file downloaded from FixID
#Loading ChEMBL-ChEBI mapping file downloaded from FixID
chebimap <- read.delim(paste0(getwd(),"/Data/CyTargetLinker/chebimap.tsv"), sep = "/t")
#Loading ChEMBL-ChEBI mapping file downloaded from FixID
chebimap <- read.delim(paste0(getwd(),"/Data/CyTargetLinker/chebimap.tsv"), sep = "\t")
View(chebimap)
View(chembl_chebi)
View(chebimap)
?rename
#Getting mapped chemicals and removing NA from df for further processing
rename(mapped_chembls,
ChEMBLid = identifier,
ChEBIid = target)
#Getting mapped chemicals and removing NA from df for further processing
mapped_chembls <- mapped_chembls %>%
rename(ChEMBLid = identifier,
ChEBIid = target)
View(chembl_chebi)
View(mapped_chembls)
chembl_chebi <- chembl_chebi %>%
rename(ChEMBLid = identifier,
ChEBIid = target)
chembl_chebi <- chembl_chebi %>%
rename(ChEMBLid = identifier,
ChEBIid = target) %>%
select(-c(identifier.source,target.source))
#Writing ChEMBL IDs to file
#ChEMBL IDs are currently mapped to ChEBI IDs using the FixID webservice manually
chembl_chebi <- read.delim(paste0(getwd(),"/Data/CyTargetLinker/chembl-chebi.tsv"),sep="\t")
chembl_chebi <- chembl_chebi %>%
rename(ChEMBLid = identifier,
ChEBIid = target) %>%
select(-c(identifier.source,target.source))
#Writing ChEMBL IDs to file
#ChEMBL IDs are currently mapped to ChEBI IDs using the FixID webservice manually
chembl_chebi <- read.delim(paste0(getwd(),"/Data/CyTargetLinker/chembl-chebi.tsv"),sep="\t")
chembl_chebi <- chembl_chebi %>%
rename(ChEMBLid = identifier,
ChEBIid = target) %>%
select(-c(identifier.source,target.source)) %>%
mutate(across(ChEBIid, ~paste("CHEBI:",.,sep="")))
mapped_chembls2 <- merge(mapped_chembls,chembl_chebi,by.x="CTL.ChEMBL",by.y="ChEMBLid",all.y=TRUE)
View(mapped_chembls2)
#Loading .tsv containing ChEBI IDs with associated ontology IDs and names
chebimap2 <- chebimap %>%
rename(ChEBIid = chemical,
ChEBIrole = role,
ChEBIrolename = rolename) %>%
mutate(ChEBIid = str_replace(ChEBIid, ".*/",""),
ChEBIrole = str_replace(ChEBIrole, ".*/",""))
View(chebimap2)
#Loading .tsv containing ChEBI IDs with associated ontology IDs and names
chebimap2 <- chebimap %>%
rename(ChEBIid = chemical,
ChEBIrole = role,
ChEBIrolename = rolename) %>%
#Renaming cols
mutate(ChEBIid = str_replace(ChEBIid, ".*/",""),
ChEBIrole = str_replace(ChEBIrole, ".*/","")) %>%
#Removing URL to get ChEBI IDs
mutate(ChEBIid = str_replace(ChEBIid, "_",":"),
ChEBIrole = str_replace(ChEBIrole,"_",":"))
View(chebimap2)
#Loading .tsv containing ChEBI IDs with associated ontology IDs and names
chebimap <- chebimap %>%
rename(ChEBIid = chemical,
ChEBIrole = role,
ChEBIrolename = rolename) %>%
#Renaming cols
mutate(ChEBIid = str_replace(ChEBIid, ".*/",""),
ChEBIrole = str_replace(ChEBIrole, ".*/","")) %>%
#Removing URL to get ChEBI IDs
mutate(ChEBIid = str_replace(ChEBIid, "_",":"),
ChEBIrole = str_replace(ChEBIrole,"_",":"))
#Replacing underscores with colons
chembl_chebi2 <- merge(chembl_chebi, chebimap, by="ChEBIid",all.x=TRUE)
View(chembl_chebi2)
#Replacing underscores with colons
chembl_chebi2 <- merge(chembl_chebi, chebimap, by="ChEBIid",all.y=TRUE)
#Replacing underscores with colons
chembl_chebi2 <- merge(chembl_chebi, chebimap, by="ChEBIid",all.x=TRUE)
View(chembl_chebi2)
?merge
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi2 <- group_by(chembl_chebi2$ChEBIid)
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi2 <- chembl_chebi2 %>%
group_by(ChEBIid) %>%
summarise(ChEBIrole = paste(ChEBIrole, sep = "; "),
ChEBIrolename = paste(ChEBIrolename, sep = "; "))
#Replacing underscores with colons
chembl_chebi2 <- merge(chembl_chebi, chebimap, by="ChEBIid",all.x=TRUE)
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi2 <- chembl_chebi2 %>%
group_by(ChEBIid)
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi2 <- chembl_chebi2 %>%
group_by(ChEBIid)
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi2 <- chembl_chebi2 %>%
group_by(ChEBIid) %>%
summarise(ChEBIrole = paste(ChEBIrole, sep = "; "),
ChEBIrolename = paste(ChEBIrolename, sep = "; "),
ChEMBLid = ChEMBLid)
#Replacing underscores with colons
chembl_chebi2 <- merge(chembl_chebi, chebimap, by="ChEBIid",all.x=TRUE)
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi2 <- chembl_chebi2 %>%
group_by(ChEBIid) %>%
reframe(ChEBIrole = paste(ChEBIrole, sep = "; "),
ChEBIrolename = paste(ChEBIrolename, sep = "; "),
ChEMBLid = ChEMBLid)
#Replacing underscores with colons
chembl_chebi2 <- merge(chembl_chebi, chebimap, by="ChEBIid",all.x=TRUE)
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi2 <- chembl_chebi2 %>%
group_by(ChEBIid,ChEMBLid) %>%
summarise(ChEBIrole = paste(ChEBIrole, sep = "; "),
ChEBIrolename = paste(ChEBIrolename, sep = "; "))
#Replacing underscores with colons
chembl_chebi2 <- merge(chembl_chebi, chebimap, by="ChEBIid",all.x=TRUE)
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi2 <- chembl_chebi2 %>%
group_by(ChEBIid,ChEMBLid) %>%
summarise(ChEBIrole = paste(ChEBIrole, sep = "; "),
ChEBIrolename = paste(ChEBIrolename, sep = "; ")) %>%
ungroup()
#Replacing underscores with colons
chembl_chebi2 <- merge(chembl_chebi, chebimap, by="ChEBIid",all.x=TRUE)
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi2 <- chembl_chebi2 %>%
group_by(ChEBIid,ChEMBLid) %>%
summarise(ChEBIrole = paste(ChEBIrole, collapse = "; "),
ChEBIrolename = paste(ChEBIrolename, collapse = "; "))
#Replacing underscores with colons
chembl_chebi <- merge(chembl_chebi, chebimap, by="ChEBIid",all.x=TRUE)
#Mapping ChEBI ontology terms to ChEBI IDs available from network mapping
chembl_chebi <- chembl_chebi2 %>%
group_by(ChEBIid,ChEMBLid) %>%
summarise(ChEBIrole = paste(ChEBIrole, collapse = "; "),
ChEBIrolename = paste(ChEBIrolename, collapse = "; "))
View(chembl_chebi)
loadTableData(chembl_chebi, data.key.column = "ChEMBLid", "node",table.key.column = "CTL.ChEMBL")
chembl_chebi <- replace_na(chembl_chebi, replace="")
?replace-na
?replace_na
chembl_chebi <- chembl_chebi %>%
mutate_all(~ifelse(is.na(.),"",.))
chembl_chebi <- chembl_chebi %>%
mutate_all(~ifelse(is.na(.),"",.))
View(chembl_chebi)
chembl_chebi <- chembl_chebi %>%
mutate_all(~str_replace_all(.,"NA",""))
View(chembl_chebi)
