other_savepath <- sprintf("%1$s/Outputs/Session-%2$s/Other/",getwd(),datetime)
dir.create(paste0(other_savepath,"WikiPathways"))
dir.create(paste0(other_savepath,"DisGeNET"))
dir.create(paste0(other_savepath,"AOP-Wiki"))
dir.create(paste0(other_savepath,"Clustering"))
dir.create(paste0(other_savepath,"CyTargetLinker"))
file.create(sprintf("Outputs/Session-%s/metadata.txt",datetime))
#Creating a new metadata file with the current date and time as suffix for easier organisation
#Such a metadata file should be generated every time this script is ran to record parameters and versions of functions or databases, including the time avoids files being overwritten if the script is run multiple times a day (can even include seconds if script is ran multiple times per minute)
metadata.add <- function(info) {
write(sapply(info, as.character), sprintf("Outputs/Session-%s/metadata.txt",datetime),append=TRUE, sep = "\n")
}
metadata.add(sysdatetime)
metadata.add(Sys.timezone())
metadata.add("")
#Adding the timezone, date, and time to the metadata
invisible(file.create(sprintf("Outputs/Session-%s/sessioninfo-%s.txt",datetime,datetime)))
writeLines(capture.output(sessionInfo()),sprintf("Outputs/Session-%s/sessioninfo-%s.txt",datetime,datetime))
#Generating and adding a sessionInfo file to the current session output folder
execution_times <- list()
start_section <- function(section_name) {
execution_times[[section_name]] <<- Sys.time()
}
end_section <- function(section_name) {
end_time <- Sys.time()
start_time <- execution_times[[section_name]]
execution_time <- end_time - start_time
# Convert elapsed time to seconds
execution_time_seconds <- as.numeric(execution_time, units = "secs")
# Convert seconds to a human-readable format with two decimal places
time_string <- format(round(execution_time_seconds, 2), nsmall = 2)
# Add "s" for seconds
time_string <- paste(time_string, "s", sep = "")
measurements <- paste(section_name, "\t", time_string, "\n", sep = "")
cat(measurements, file = sprintf("Outputs/Session-%s/execution-times.txt",datetime), append = TRUE)
# Remove start time from the list
execution_times[[section_name]] <- NULL
}
cytoscapePing()
cytoscapeVersionInfo()
#Checking if Cytoscape is running and version info
metadata.add(capture.output(cytoscapeVersionInfo()))
checkinstall.app <- function(app) {
status_string <- getAppStatus(app)
#Getting install status of app
words <- strsplit(status_string, " ")[[1]]
last_word <- tail(words, 1)
#getAppStatus returns a character string instead of a logical value, so the last word (usually either "Installed" or "Uninstalled") from the output is checked
if (last_word == "Installed") {
print(sprintf("App %s is already installed.",app))
} else {
installApp(app)
print(sprintf("Installed app %s.",app))
}
}
#Function to check whether required Cytoscape apps are installed and installing them if not
applist <- c("stringApp","clusterMaker2","yFiles Layout Algorithms","CyTargetLinker")
#WikiPathways v.3.3.10
#DisGeNET-app v.7.3.0
#CyTargetLinker v. 4.1.0
#stringApp v. 2.0.2
#BridgeDb v.1.2.0
#clusterMaker2 v.2.3.4
lapply(applist,checkinstall.app)
#Checking and installing (if required) necessary Cytoscape apps
lapply(applist,getAppInformation)
metadata.add("Required Cytoscape apps and versions:")
invisible(metadata.add(print(lapply(applist,getAppInformation))))
metadata.add("")
dir.create(paste0(getwd(),"/BridgeDb"))
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "BridgeDb mapping file not detected. Download BridgeDb mapping file for Homo sapiens (800.4 MB)? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=paste0(getwd(),"/BridgeDb"))
message("BridgeDb mapping file downloaded to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
#Downloading the BridgeDb bridge file for Homo sapiens identifiers to the repo
#The directory is added to .gitignore to avoid uploading it to GitHub
#This is a relatively large (800MB) file - downloading it once is sufficient
metadata.add("BridgeDb")
metadata.add("Homo sapiens bridge version 108")
metadata.add("")
# FUNCTION DICTIONARY-------------------------------------------------------------------------------------------------------------------
.defaultBaseUrl <- 'http://127.0.0.1:1234/v1'
#Defining the default base URL found in the RCy3 source as R object for altmergeNetworks
altmergeNetworks <- function(               sources = NULL,
title = NULL,
operation = "union",
nodeKeys = NULL,
nodeMergeMap = NULL,
nodesOnly = FALSE,
edgeKeys = NULL,
edgeMergeMap = NULL,
networkMergeMap = NULL,
inNetworkMerge = TRUE,
base.url = .defaultBaseUrl) {
cmd.string <- 'network merge' # a good start
# sources must be suppled
if(is.null(sources)) {
message("Missing sources!")
return(NULL)
} else {
sources.str <- paste(sources, collapse = ",")
cmd.string <- paste0(cmd.string,' sources="',sources.str,'"')
}
# defaults
cmd.string <- paste0(cmd.string,' operation=',operation)
cmd.string <- paste0(cmd.string,' nodesOnly=',nodesOnly)
cmd.string <- paste0(cmd.string,' inNetworkMerge=',inNetworkMerge)
# optional args
if(!is.null(title))
cmd.string <- paste0(cmd.string,' netName="',title,'"')
if(!is.null(nodeKeys))
cmd.string <- paste0(cmd.string,' nodeKeys="',paste(nodeKeys, collapse = ","),'"')
if(!is.null(edgeKeys))
cmd.string <- paste0(cmd.string,' edgeKeys="',paste(edgeKeys, collapse = ","),'"')
if(!is.null(nodeMergeMap)){
nodeMergeMap.str <- paste(nodeMergeMap, collapse = ",")
nodeMergeMap.str <- gsub("c\\(", "{", nodeMergeMap.str)
nodeMergeMap.str <- gsub("\\)", "}", nodeMergeMap.str)
cmd.string <- paste0(cmd.string,' nodeMergeMap="',nodeMergeMap.str,'"')
}
if(!is.null(edgeMergeMap)){
edgeMergeMap.str <- paste(edgeMergeMap, collapse = ",")
edgeMergeMap.str <- gsub("c\\(", "{", edgeMergeMap.str)
edgeMergeMap.str <- gsub("\\)", "}", edgeMergeMap.str)
cmd.string <- paste0(cmd.string,' edgeMergeMap="',edgeMergeMap.str,'"')
}
if(!is.null(networkMergeMap)){
networkMergeMap.str <- paste(networkMergeMap, collapse = ",")
networkMergeMap.str <- gsub("c\\(", "{", networkMergeMap.str)
networkMergeMap.str <- gsub("\\)", "}", networkMergeMap.str)
cmd.string <- paste0(cmd.string,' networkMergeMap="',networkMergeMap.str,'"')
}
res.data <- commandsPOST(cmd.string, base.url = base.url)
if(!is.null(res.data$SUID))
return(res.data$SUID)
else
return(res.data)
}
#Normally, RCy3's 'mergeNetworks' function would be used to unify imported networks into one supernetwork
#This function does however not work on the latest RCy3 release (v.2.22.1), but does work when running the script on RCy3 v.2.14.2
#RCy3 2.14.2 requires R v.4.1.3, requiring the entire script to run on an old version of R for one function that is used once
#Here, we redefine the function using the source code from RCy3 v.2.14.2 and simply use this alternate function to merge networks
sparqlquery <- function(endpoint,queryfile,output) {
if (tolower(endpoint) %in% c("WikiPathways","wikipathways","wp","WP")) {
file_path <- paste0(getwd(),sprintf("/Data/WikiPathways/%s",queryfile))
#Specifying file path to the .txt file containing the query
base_url <- "https://sparql.wikipathways.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else if (tolower(endpoint) %in% c("AOP-Wiki", "aopwiki","aop-wiki","AOPWiki")) {
file_path <- paste0(getwd(),sprintf("/Data/AOP-Wiki/%s",queryfile))
#Specifying file path to the .txt file containing the query
base_url <- "https://aopwiki.rdf.bigcat-bioinformatics.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else if (tolower(endpoint) %in% c("DisGeNET","DGN","disgenet","Disgenet")) {
file_path <- paste0(getwd(),sprintf("/Data/DisGeNET/%s",queryfile))
#Specifying path to the .txt file containing the query
base_url <- "http://rdf.disgenet.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else
print("Error: Please specify WikiPathways, DisGeNET or AOP-Wiki as endpoint.")
querybody <- paste(readLines(file_path), collapse = "")
#Reading a text file containing a SPARQL query stored in the repo
encoded_query <- URLencode(querybody)
#Encoding the query as URL
full_url <- paste0(base_url, encoded_query, "&format=text/html&timeout=0&signal_void=on")
#Joining the query from the text file and the base URL and adding that output is desired as HTML
html <- read_html(full_url)
#Sending the query to the SPARQL endpoint and extracting as HTML
if(tolower(endpoint) %in% c("DisGeNET","DGN","disgenet","Disgenet")) {
tablebody <- html %>%
html_element("body") %>%
html_element("table") }
#DisGeNET has a slightly different HTML structure than AOP-Wiki and WikiPathways, so navigating to the table is done accordingly
else
tablebody <- html %>%
html_element("body") %>%
html_element("div") %>%
html_element("table")
#Navigating to the table output by the SPARQL query
assign(output, html_table(tablebody), envir=.GlobalEnv)
#Getting the output as tibble
}
#Function to send a SPARQL query defined in a local text file to the endpoint and extract to desired dataframe
#Will not work if query includes comments ('# this is a comment') due to HTML conversion
createNodeSource <- function(source,doi=NULL) {
if (source == "fromDisGeNET") {
networkname <- getNetworkName()
nodetable <- paste0(networkname," default  node")
}
#Networks imported from WikiPathways have a type in the node table designations, as they have 2 spaces between "default" and "node" instead of one
#This check determines which node table name format is to be applied depending on the source (WikiPathways or other)
else {
networkname <- getNetworkName()
nodetable <- paste0(networkname," default node")
}
commandsRun(sprintf("table create column columnName=fromWikiPathways table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromDisGeNET table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromPublication table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=Publication.doi table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromSTRING table=%s type=string",nodetable))
#Creating a new column for each source used for all networks
if ( source == "STRINGnode") {
commandsRun(sprintf('table set values columnName=%1$s handleEquations=false rowList="selected:true" table=%2$s value=1',source,nodetable))
}
else {
commandsRun(sprintf("table set values columnName=%1$s handleEquations=false rowList=all table=%2$s value=1",source,nodetable))
#Filling the new column of the corresponding source with 1 to indicate which source the node is imported from
}
if (!is.null(doi)) {
commandsRun(sprintf("table set values columnName=Publication.doi handleEquations=false rowList=all table=%1$s value=%2$s",nodetable,doi))
#Adding doi for literature used if provided
}
}
#Function to create new column in node table specifying origin of network/node
# SCHIZOPHRENIA =======================================================================================================================
## IMPORTING AND MERGING ---------------------------------------------------------------------------------------------------------------
start_section("Importing and merging")
sparqlquery("DisGeNET","disgenet-query.txt","gda")
#Querying the DisGeNET SPARQL endpoint for genes associated to schizophrenia from curated sources
gda$disgenet_curated <- str_extract(gda$source, "(?<=/)[^/]*$")
gda$Entrez_gene <- str_extract(gda$gene, "(?<=/)[^/]*$")
gda$HGNC_symbol <- str_extract(gda$symbol, "(?<=/)[^/]*$")
gda <- subset(gda, select=c(disgenet_curated,Entrez_gene,HGNC_symbol,gdascore))
#cleaning data
gda <- gda %>%
group_by(Entrez_gene, HGNC_symbol, gdascore) %>%
mutate(disgenet_curated = paste(disgenet_curated, collapse = "; ")) %>%
distinct()
gda <- gda[!duplicated(gda$Entrez_gene),]
#Concatenating to avoid duplicate gene rows if they are confirmed by multiple sources
mapper <- loadDatabase(bridgedb_dir)
#Loading bridgedb database
input <- data.frame(
source = rep("H", length(gda[, 3])),
identifier = gda[, 3]
)
#Making a new df to be used as input for bridgedb
#Map HGNC symbol
input <- input %>%
rename(source=source,
identifier=HGNC_symbol)
#Renaming cols for maps function compability
gda_map <- maps(mapper,input,"En")
#Mapping from HGNC to Ensembl
gda <- merge(gda,gda_map,by.x="HGNC_symbol",by.y="identifier",all.x=TRUE)
#Merging the GDA and mapping tables; some HGNC symbols can't be matched to Ensembl IDs but are still retained
gda <- subset(gda, select=c(HGNC_symbol,mapping,Entrez_gene,disgenet_curated,gdascore))
#Cleaning df
gda <- gda %>%
rename(Ensembl=mapping)
man_map <- read.delim(paste0(getwd(),"/Data/DisGeNET/manual-maps.txt"),sep="\t")
#Reading a file containing manual mappings for some of the missing Ensembl ID
gda <- merge(gda, man_map, by="HGNC_symbol",all.x=TRUE)
#Merging the manual map and the gda df based on HGNC symbol as key column
gda$Ensembl.x[is.na(gda$Ensembl.x)] <- gda$Ensembl.y[is.na(gda$Ensembl.x)]
#Merging the Ensembl cols
gda <- select(gda, -Ensembl.y)
#Removing superfluous Ensembl col from manual mapping df
gda <- gda %>%
rename(Ensembl = Ensembl.x)
gda$HGNC_symbol_source <- gda$HGNC_symbol
#Creating a duplicated Ensembl column for Cytoscape import
#Cleaning and renaming df
gda <- mutate_all(gda, ~ifelse(is.na(.),"",.))
#Replacing NA with empty strings for Cytoscape compatibility
write.table(gda,file=paste0(other_savepath,"DisGeNET/gda.tsv"),quote=FALSE,sep="\t",row.names=FALSE)
commandsRun(sprintf('network import file columnTypeList=sa,sa,sa,sa,sa,s delimiters=\\t file=%s firstRowAsColumnNames=true startLoadRow=1',paste0(other_savepath,"DisGeNET/gda.tsv")))
Sys.sleep(0.5)
renameNetwork("DisGeNET network")
createNodeSource("fromDisGeNET")
Sys.sleep(0.5)
metadata.add("DisGeNET")
metadata.add("DisGeNET SPARQL endpoint metadata:")
metadata.add(paste0("DisGeNET nodes: ",getNodeCount()))
metadata.add("")
sparqlquery("wp","metadataquery.txt","WikiPathways-SPARQL-metadata")
#Getting the metadata of the endpoint used for the WikiPathways SPARQL queries
metadata.add("WikiPathways")
metadata.add("WikiPathways SPARQL endpoint metadata:")
metadata.add(`WikiPathways-SPARQL-metadata`)
#Adding the fetched metadata to the metadata file for the session
#It is technically possible that the metadata would describe an earlier version of the RDF if it is updated while the script runs but this is unlikely
sparqlquery("wp","pathwayquery.txt","wp_pathwaylist")
#Getting a list of pathways corresponding to a keyword as per defined in the query
writeLines(wp_pathwaylist[["PWID"]], con=paste0(other_savepath,"WikiPathways/automaticpathways.txt"))
#Saving the output pathway list to file
manualpathways <- readLines("Data/WikiPathways/Pathwaylists/manualpathways.txt")
#Reading a file containing a list of manually selected pathways
allpathways <- c(wp_pathwaylist[["PWID"]],manualpathways)
#Joining the list of manually and automatically selected pathways together
allpathways_URL <- paste0("<",allpathways,">")
#Adding <> around all entries for easier use in SPARQL queries
#URLs need to be surrounded by <> to be recognised as such
writeLines(allpathways_URL, con=paste0(other_savepath,"WikiPathways/allpathways.txt"))
#Writing list of all pathways in SPARQL URL format to file
sparqlquery("wp","nodequery.txt","wp_nodelist")
#Making a SPARQL query to the endpoint to get all nodes associated with a list of pathways
if (any(grepl("identifiers\\.org", wp_nodelist$Identifier))) {
# Checking whether the 'Identifier' column contains the identifiers.org URL
#This is to avoid issues later when the identifiers.org part is removed and the code is reran
wp_nodelist$WPNodeIDType <- gsub(".*/([^/]+)/.*", "\\1", wp_nodelist$Identifier)
#If 'identifiers.org' is still in the column, extract part of the string into a new column to see what type the identifier is
} else {
# If 'identifiers.org' is not found, do nothing
}
wp_nodelist[] <- lapply(wp_nodelist, function(x) str_replace_all(x, "https://identifiers\\.org/([^/]+)/", ""))
#Selecting and removing "https://identifiers.org/xyz" from every row in the df for improved readability
wp_nodelist$CNVassociated <- ifelse(grepl("copy number | CNV | deletion",wp_nodelist$PathwayTitle), 1, NA)
#Adding a new binary column showing if a given node is associated with a CNV based on pathway title
wp_nodelist$WPNodeID <- wp_nodelist$Identifier
#Generating a duplicate node identifier column since the original column will be lost during Cytoscape import due to it being selected as source column
write.table(wp_nodelist, file=paste0(other_savepath,"WikiPathways/nodelist.tsv"), quote=FALSE, sep="\t", row.names=FALSE)
#Writing the modified file for Cytoscape import
commandsRun(sprintf('network import file columnTypeList="sa,sa,s,sa,sa,sa,sa,sa" file=%s firstRowAsColumnNames=true rootNetworkList=-- Create new network collection -- startLoadRow=1 delimiters=\\t', paste0(other_savepath,"WikiPathways/nodelist.tsv")))
#Importing a list of nodes from the output of a WikiPathways SPARQL query (get all nodes in pathways matching the keyword 'Schizophrenia' and some manually selected pathways)
Sys.sleep(0.5)
#Adding sys.sleep to give Cytoscape sufficient time to import the file as network; otherwise, renaming doesn't always work since no network is selected until the import is complete
renameNetwork("WikiPathways nodes")
sparqlquery("wp","edgequery.txt","wp_edgelist")
#Making a SPARQL query to the endpoint to get a list of source-target pairs from selected pathways
wp_edgelist[] <- lapply(wp_edgelist, function(x) str_replace_all(x, "https://identifiers\\.org/([^/]+)/", ""))
#Selecting and removing "https://identifiers.org/xyz" from every row in the df for improved readability
edge_df <- wp_edgelist[grepl("Interaction",wp_edgelist$source) | grepl("Interaction",wp_edgelist$target),]
#Extracting rows containing "Interaction" in either the source or target column
#Interaction nodes represent phosphorylation and the like and are not suitable for the network
#They can still provide information about the connection of gene or other nodes so they can't just be deleted either
#If an Interaction node is connected to two or more non-interaction nodes, these nodes should be connected to each other, and the interaction node can be deleted
interaction_freq <- table(edge_df$target)
edge_df_filtered <- edge_df[edge_df$target %in% names(interaction_freq[interaction_freq > 1]),]
#Counting if a certain interaction occurs more than once; this implies that it is connected to more than one non-interaction node
unique_targets <- unique(edge_df_filtered$target)
for (target_val in unique_targets) {
# Identify rows with duplicate target values
rows_with_duplicate_target <- which(edge_df_filtered$target == target_val)
if (length(rows_with_duplicate_target) > 1) {
# Select one of the source values
source_val_to_transpose <- edge_df_filtered$source[rows_with_duplicate_target[1]]
# Transpose the source value to the target column in the row of the remaining source value
edge_df_filtered$target[rows_with_duplicate_target[-1]] <- source_val_to_transpose
# Remove duplicate rows
edge_df_filtered <- edge_df_filtered[-rows_with_duplicate_target[1], ]
}
}
#Transposing the non-identifier nodes for source-target pairs; if two nodes are associated with the same interaction, they become source-target pairs
wp_edgelist <- wp_edgelist <- wp_edgelist[!grepl(".*interaction.*", wp_edgelist$source, ignore.case = TRUE) &
!grepl(".*interaction.*", wp_edgelist$target, ignore.case = TRUE), ]
#Removing any row containing "Interaction"
wp_edgelist <- rbind(wp_edgelist,edge_df_filtered)
#Appending the new source-target pairs to the original edge list
write.table(wp_edgelist, file=paste0(other_savepath,"WikiPathways/edgelist.tsv"), quote=FALSE, sep="\t", row.names=FALSE)
#Writing the modified file for Cytoscape import
commandsRun(sprintf('network import file columnTypeList="sa,s,t" file=%s firstRowAsColumnNames=true rootNetworkList=-- Create new network collection -- startLoadRow=1 delimiters=\\t', paste0(other_savepath,"WikiPathways/edgelist.tsv")))
#Importing a list of source-target pairs from selected pathways from the ouput of a WikiPathways SPARQL query
Sys.sleep(0.5)
#Adding sys.sleep to give Cytoscape sufficient time to import the file as network; otherwise, renaming doesn't always work since no network is selected until the import is complete
renameNetwork("WikiPathways edges")
altmergeNetworks(sources = c("WikiPathways nodes","WikiPathways edges"),
title = "WikiPathways networks",
operation = "union",
nodeKeys=c("WPNodeID","name"))
#Union merging the node and edge networks to extend the node list with corresponding edges
Sys.sleep(0.5)
createNodeSource("fromWikiPathways")
metadata.add(paste0("WikiPathways nodes: ",getNodeCount()))
metadata.add("")
deleteNetwork('WikiPathways nodes')
deleteNetwork('WikiPathways edges')
Sys.sleep(1)
#Pausing the script for 1 second - when letting the script run without this, the publication source creation fails
commandsRun(sprintf("network import file columnTypeList='sa,sa,source,sa,sa,sa,sa' file=%s firstRowAsColumnNames=true rootNetworkList=-- Create new network collection -- startLoadRow=1", paste0(getwd(),"/Data/Publications/Trubetskoy.txt")))
#Importing network from file
#List of 120 genes implicated in Trubetskoy et al., doi: 10.1038/s41586-022-04434-5
commandsRun("table rename column columnName=Ensembl.ID newColumnName=Ensembl table=Trubetskoy.txt default node")
#Renaming the Ensembl.ID column from the dataset to Ensembl for coherence with networks from other sources
commandsRun("table rename column columnName=Index.SNP newColumnName=snpID table=Trubetskoy.txt default node")
createNodeSource("fromPublication","10.1038/s41586-022-04434-5")
#Adding literature as  source to all imported nodes and adding the doi of the corresponding paper
renameNetwork("Trubetskoy risk genes")
#Renaming the newly imported network
metadata.add("Publications")
metadata.add("Trubetskoy et al. doi: 10.1038/s41586-022-04434-5")
metadata.add(paste0("Publication nodes: ",getNodeCount()))
metadata.add("")
sparqlquery("AOP-Wiki","metadataquery.txt","aopwikimetadata")
aopwikimetadata <- paste(aopwikimetadata$dataset, aopwikimetadata$date, sep ="\t")
metadata.add("AOP-Wiki")
metadata.add("AOP-Wiki SPARQL endpoint metadata:")
metadata.add(paste("Dataset","Date",sep="\t"))
metadata.add(aopwikimetadata)
metadata.add("")
networklist <- getNetworkList()
setCurrentNetwork(networklist[[1]])
for(i in 1:length(networklist)) {
current <- getNetworkName()
altmergeNetworks(c(current,networklist[[i]]), paste(current,networklist[[i]]),"union",inNetworkMerge = TRUE,nodeKeys=c("Ensembl","Ensembl"))
}
#Looping through the network list to merge all currently open networks with each other, creating one large unified network
renameNetwork("Schizophrenia supernetwork")
networklist <- getNetworkList()
snw_scz <- getNetworkName()
#Getting the name of the unified network to preserve it from deletion
lapply(networklist[networklist != snw_scz],deleteNetwork)
#Deleting all networks besides newly generated unified network
snw_ensembl <- getTableColumns("node","Ensembl")
#Getting all values in the Ensembl column of the supernetwork
input <- data.frame(
source = rep("En", length(snw_ensembl[, 1])),
identifier = snw_ensembl[, 1]
)
#Making a new df to be used as input for bridgedb
#Map Ensembl ID
snw_map <- maps(mapper,input,"H")
#Mapping from Ensembl to HGNC
snw_map <- select(snw_map, c("identifier", "mapping"))
snw_map <- rename(snw_map,
Ensembl = identifier,
HGNCsymbol = mapping)
#Selecting and renaming relevant columns from bridgeDb mapping output
loadTableData(snw_map,
data.key.column = "Ensembl",
table = "node",
table.key.column = "Ensembl")
#loading HGNC names for Ensembl IDs in supernetwork back to node table
snw_nostring_edges <- getEdgeCount()
metadata.add(paste0("Total nodes in supernetwork: ",getNodeCount()))
metadata.add("")
exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW"),"CX", network = snw_scz, overwriteFile=TRUE)
#Exporting the supernetwork as cx file
end_section("Importing and merging")
library(ggplot2)
stable <- getTableColumns("node",c("fromDisGeNET","fromPublication","fromWikiPathways"))
stable$count_ones <- rowSums(!is.na(stable))
combination_counts <- table(stable$count_ones)
sourcenames <- c("Appears in one source","Appears in two sources","Appears in three sources")
visframe <- data.frame(sourcenames,combination_counts)
visframe <- select(visframe, -Var1)
ggplot(visframe,aes(x=sourcenames,y=Freq)) +
geom_bar(stat="identity", fill = 'skyblue') +
labs(x = sourcenames,  y = "Counts", title = "Node counts by source") +
scale_y_continous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
ggplot(visframe,aes(x=sourcenames,y=Freq)) +
geom_bar(stat="identity", fill = 'skyblue') +
labs(x = sourcenames,  y = "Counts", title = "Node counts by source") +
scale_y_continuous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
?geom_bar
ggplot(visframe,aes(x=sourcenames,y=Freq)) +
geom_bar(stat="identity", fill = 'skyblue') +
labs(  y = "Counts", title = "Node counts by source") +
scale_y_continuous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
ggplot(visframe,aes(x=sourcenames,y=Freq)) +
geom_bar(stat="identity", fill = 'skyblue') +
labs(x=NULL,  y = "Counts", title = "Node counts by source") +
scale_y_continuous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
visframe
ggplot(visframe,aes(x=factor(sourcenames,levels=c("Appears in one source","Appears in two sources","Appears in three sources")),y=Freq)) +
geom_bar(stat="identity", fill = 'skyblue') +
labs(x=NULL,  y = "Counts", title = "Node counts by source") +
scale_y_continuous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
ggplot(visframe,aes(x=factor(sourcenames,levels=c("Appears in one source","Appears in two sources","Appears in three sources")),y=Freq)) +
geom_bar(stat="identity", fill = 'skyblue', width=0.5) +
labs(x=NULL,  y = "Counts", title = "Node counts by source") +
scale_y_continuous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
sourcenames <- c("One source","Two sources","Three sources")
visframe <- data.frame(sourcenames,combination_counts)
visframe <- select(visframe, -Var1)
ggplot(visframe,aes(x=factor(sourcenames,levels=c("One source","Two sources","Three sources")),y=Freq)) +
geom_bar(stat="identity", fill = 'skyblue', width=0.5) +
labs(x=NULL,  y = "Counts", title = "Node counts by source") +
scale_y_continuous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
ggplot(visframe,aes(x=factor(sourcenames,levels=c("One source","Two sources","Three sources")),y=Freq)) +
geom_bar(stat="identity", fill = 'green4', width=0.5) +
labs(x=NULL,  y = "Counts", title = "Node counts by source") +
scale_y_continuous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
visframe <- select(visframe, -Var1)
ggplot(visframe,aes(x=factor(sourcenames,levels=c("One source","Two sources","Three sources")),y=Freq)) +
geom_bar(stat="identity", fill = 'darkorange2', width=0.5) +
labs(x=NULL,  y = "Counts", title = "Node counts by source") +
scale_y_continuous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
View(visframe)
visframe <- visframe %>%
mutate(relative_freq = Freq / sum(Freq))
ggplot(visframe,aes(x=factor(sourcenames,levels=c("One source","Two sources","Three sources")),y=relative_freq)) +
geom_bar(stat="identity", fill = 'darkorange2', width=0.5) +
labs(x=NULL,  y = "Counts", title = "Node counts by source") +
scale_y_continuous(labels=scales::percent_format(scale = 1), expand = c(0,0)) +
theme_minimal()
ggplot(visframe,aes(x=factor(sourcenames,levels=c("One source","Two sources","Three sources")),y=Freq)) +
geom_bar(stat="identity", fill = 'darkorange2', width=0.5) +
labs(x=NULL,  y = "Counts", title = "Node counts by source") +
scale_y_continuous(trans="log10", breaks=c(1,10,100,1000,2000)) +
theme_minimal()
