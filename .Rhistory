write.table(wp_edgelist, file=paste0(getwd(),"/Data/WikiPathways/edgelist.tsv"), quote=FALSE, sep="\t", row.names=FALSE)
#Writing the modified file for Cytoscape import
commandsRun(sprintf('network import file columnTypeList="sa,s,t" file=%s firstRowAsColumnNames=true rootNetworkList=-- Create new network collection -- startLoadRow=1 delimiters=\\t', paste0(getwd(),"/Data/WikiPathways/edgelist.tsv")))
#Importing a list of source-target pairs from selected pathways from the ouput of a WikiPathways SPARQL query
Sys.sleep(0.5)
#Adding sys.sleep to give Cytoscape sufficient time to import the file as network; otherwise, renaming doesn't always work since no network is selected until the import is complete
renameNetwork("WikiPathways edges")
altmergeNetworks(sources = c("WikiPathways nodes","WikiPathways edges"),
title = "WikiPathways networks",
operation = "union",
nodeKeys=c("WPNodeID","name"))
#Union merging the node and edge networks to extend the node list with corresponding edges
Sys.sleep(0.5)
createNodeSource("fromWikiPathways")
deleteNetwork('WikiPathways nodes')
deleteNetwork('WikiPathways edges')
Sys.sleep(1)
#Pausing the script for 1 second - when letting the script run without this, the publication source creation fails
commandsRun(sprintf("network import file columnTypeList='sa,sa,source,sa,sa,sa,sa' file=%s firstRowAsColumnNames=true rootNetworkList=-- Create new network collection -- startLoadRow=1", paste0(getwd(),"/Data/Publications/Trubetskoy.txt")))
#Importing network from file
#List of 120 genes implicated in Trubetskoy et al., doi: 10.1038/s41586-022-04434-5
commandsRun("table rename column columnName=Ensembl.ID newColumnName=Ensembl table=Trubetskoy.txt default node")
#Renaming the Ensembl.ID column from the dataset to Ensembl for coherence with networks from other sources
commandsRun("table rename column columnName=Index.SNP newColumnName=snpID table=Trubetskoy.txt default node")
createNodeSource("fromPublication","10.1038/s41586-022-04434-5")
#Adding literature as  source to all imported nodes and adding the doi of the corresponding paper
renameNetwork("Trubetskoy risk genes")
#Renaming the newly imported network
metadata.add("Publications")
metadata.add("Trubetskoy et al. doi: 10.1038/s41586-022-04434-5")
metadata.add("")
# networklist.dup <- getNetworkList()
# dup.filter <- function(input,suffix) {
#   filtered_list <- input[substr(input, nchar(input) - 1,nchar(input))==suffix]}
# duplicates <- dup.filter(networklist.dup,"_1")
#   #Getting duplicate networks (Cytoscape marks duplicate networks with a "_1" suffix to the network name)
# delete.dupes <- function(nw) {
#  setCurrentNetwork(nw)
#   deleteNetwork()
# }
# lapply(duplicates,delete.dupes)
#Selecting and deleting duplicate networks
networklist <- getNetworkList()
setCurrentNetwork(networklist[[1]])
for(i in 1:length(networklist)) {
current <- getNetworkName()
altmergeNetworks(c(current,networklist[[i]]), paste(current,networklist[[i]]),"union",inNetworkMerge = TRUE,nodeKeys=c("Ensembl","Ensembl"))
}
#Looping through the network list to merge all currently open networks with each other, creating one large unified network
renameNetwork("Schizophrenia supernetwork")
networklist <- getNetworkList()
snw_scz <- getNetworkName()
#Getting the name of the unified network to preserve it from deletion
lapply(networklist[networklist != snw_scz],deleteNetwork)
#Deleting all networks besides newly generated unified network
exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW"),"CX", network = snw_scz, overwriteFile=TRUE)
#Exporting the supernetwork as cx file
#wpids <- c("4875","5412","4222","4942","5408","5402","5346","5405","5406","5407","4940","4905","5398","5399","4906","4657","4932")
#sczcnv <- sapply(wpids, function(k) paste0("WP",k))
#Manually adding relevant SCZ CNV pathways from WikiPathways
#keyword.wp <- "Schizophrenia"
#getPathways.wp(keyword.wp)
#lapply(c(Schizophrenia_wpids,sczcnv), import)
#Importing WP pathways (both manually added and by keyword). Also adds "WikiPathways" as NodeSource column to node table
#metadata.add(paste("WikiPathways keywords:",keyword.wp))
#metadata.add(paste("WikiPathways manually by ID:",paste(wpids,collapse =", ")))
#metadata.add(paste("WikiPathways queried species:",paste(queryspecies.wp,collapse = ", ")))
#Adding the keyword and species used to filter the WikiPathways query to the metadata file
#metadata.add("")
## FILTERING NETWORK ------------------------------------------------------------------------------------------------------------------
# createColumnFilter(filter.name="type.label",column="Type","Label","IS",apply=FALSE)
# createColumnFilter(filter.name="type.anchor",column="Type","Anchor","IS",apply=FALSE)
# createColumnFilter(filter.name="type.group",column="Type","Group","IS",apply=FALSE)
# createColumnFilter(filter.name="disease.name",column="diseaseName","Schizophrenia","IS")
# createCompositeFilter(filter.name="type.label.anchor.group",c("type.label","type.anchor","type.group","disease.name"),"ANY")
#deleteSelectedNodes()
#Creating filters and deleting columns in the node table that are not relevant to the supernetwork (leftovers from import sources)
#renameNetwork("SCZ_SNW_filtered")
#snw_scz_filtered <- getNetworkName()
#exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW_filtered"),"CX", network = snw_scz_filtered, overwriteFile=TRUE)
#Exporting the filtered supernetwork as cx file and tagging it with the time and date made to match with metadata file
## STRING --------------------------------------------------------------------------------------------------------------------------
commandsRun('string stringify colDisplayName=name column=Ensembl compoundQuery=true cutoff=0.9 includeNotMapped=true  networkType="full STRING network" species="Homo sapiens" networkNoGui=current')
# commandsRun('string expand additionalNodes=1000 network=current nodeTypes="Homo sapiens" selectivityAlpha=0.9')
#   #STRINGifying and expanding the network with a 0.9 confidence cutoff (curated information)
# createNodeSource("fromSTRING")
#   #Tagging the newly added nodes as having been sourced from STRING
# mapTableColumn("stringdb::canonical name","Human","Uniprot-TrEMBL","Ensembl",force.single=TRUE)
#   #Mapping stringdb canonical names (Uniprot-TrEMBL identifiers) to Ensembl gene identifiers
#   #This step generates a second Ensembl column ('Ensembl (1)') with ENSG identifiers for the STRING-imported nodes
# renameTableColumn("Ensembl (1)","Ensembldup")
#   #Renaming the duplicate Ensembl column for easier handling
# renameNetwork("SCZ_SNW_filtered_STRING")
# snw_scz_filtered_string <- getNetworkName()
# stringified_nodetable <- paste0(nw_savepath,sprintf("%s node table.csv",snw_scz_filtered_string))
#   #Saving the file path to the node table for easier reading
# commandsRun(sprintf('table export options=CSV outputFile=%1$s table="%2$s default  node"',stringified_nodetable,snw_scz_filtered_string))
#   #Exporting the node table as .csv file to the current session's "Network" folder
# read_stringified_nodetable <- read.csv(stringified_nodetable)
#   #saving the node table as object
# read_stringified_nodetable$Ensembl <- ifelse(read_stringified_nodetable$Ensembl == read_stringified_nodetable$Ensembldup, as.character(read_stringified_nodetable$Ensembl),
#                                ifelse(is.na(read_stringified_nodetable$Ensembl) | read_stringified_nodetable$Ensembl =="",as.character(read_stringified_nodetable$Ensembldup),
#                                       ifelse(is.na(read_stringified_nodetable$Ensembldup) | read_stringified_nodetable$Ensembldup=="",as.character(read_stringified_nodetable$Ensembl),"No Match")))
#   #As the identifier mapping from STRING ENSP to ENSG identifiers generates a second Ensembl column, they are merged into the originial Ensembl column if the contents of the cell match or either is blank
# read_stringified_nodetable = subset(read_stringified_nodetable,select= -Ensembldup)
#   #Removing the duplicate Ensembl column from the table
# write.csv(read_stringified_nodetable, file=stringified_nodetable)
#   #Overwriting the previously exported table with the version containing the merged Ensembl column
# renameTableColumn("@id","X.id")
#   #Renaming the '@id' column in the Cytoscape table to avoid issues when reimporting the .csv (@id is automatically converted to X.id in the CSV)
# loadTableData(read_stringified_nodetable,data.key.column="X.id",table.key.column="X.id")
#   #Reimporting the .csv with the merged Ensembl column to the network as to have ENSG identifiers for almost all nodes
mapTableColumn("Ensembl","Human","Ensembl","HGNC")
#   #Generating a new column 'HGNC' from Ensembl identifiers - easier and less error-prone than merging various name columns from different import sources
renameTableColumn("HGNC","Name2")
#Renaming the new 'HGNC' column to 'Name2', which is now to be used as default name column. ('shared name' and 'name' columns are immutable and cannot be deleted or renamed)
marked_cols <- as.list(getTableColumnNames()[!(getTableColumnNames() %in% c("selected","name.copy" ,"SUID","shared name","name","fromDisGeNET","fromWikiPathways","Ensembl","fromPublication","Publication.doi","fromSTRING","CNVassociated","PathwayID","NodeID","NodeIDType","snpID","Name2","DisGeNETname"))])
lapply(marked_cols, function(column) {
deleteTableColumn(column=column)
})
#Filtering columns
renameNetwork("SCZ_SNW_STRING")
scz_snw_string <- getNetworkName()
exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW__STRING"),"CX",network=scz_snw_string,overwriteFile=TRUE)
#Exporting the filtered, stringified supernetwork as cx file and tagging it with the time and data to match with the metadata file
## CLUSTERING ----------------------------------------------------------------------------------------------------------------------
createColumnFilter(filter.name="delete.noensembl", column="Ensembl","ENSG","DOES_NOT_CONTAIN")
deleteSelectedNodes()
#Filtering out nodes that do not have an ENSG Ensembl identifier mapped to them
# marked_cols <- as.list(getTableColumnNames()[!(getTableColumnNames() %in% c("selected","name.copy" ,"SUID","shared name","name","Name2","fromDisGeNET","fromWikiPathways","Ensembl","fromPublication","Publication.doi","fromSTRING","CNVassociated"))])
# lapply(marked_cols, function(column) {
#   deleteTableColumn(column=column)
# })
#Deleting all the columns besides immutable columns, Ensembl, name, and source columns
metadata.add("GLay Clustering")
metadata.add(capture.output(commandsRun('cluster glay clusterAttribute=__glayCluster createGroups=false network=current restoreEdges=true showUI=true undirectedEdges=true')))
#Clustering the network using the GLay community cluster from the clusterMaker Cytoscape app and recording outcome to metadata
renameNetwork("SCZ_SNW_filtered_STRING_clustered")
renameTableColumn('__glayCluster','gLayCluster')
#Renaming the newly generated gLayCluster column as the original name with two underscores is not recognized during gene ontology
snw_scz_filtered_string_clustered <- getNetworkName()
clustered_nodetable <- paste0(nw_savepath,sprintf("/%s node table.csv",snw_scz_filtered_string_clustered))
#Saving the file path to the node table for easier reading (note the double space between node and table)
commandsRun(sprintf('table export options=CSV outputFile=%1$s table="%2$s default  node"',clustered_nodetable,snw_scz_filtered_string_clustered))
#Exporting the node table as .csv file to the current session's "Network" folder
exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW_filtered_STRING_clustered"),"CX",network=snw_scz_filtered_string_clustered,overwriteFile=TRUE)
#Exporting the filtered, stringified, clustered supernetwork as cx file and tagging it with the time and data to match with the metadata file
read_clustered_nodetable <- read.csv(clustered_nodetable)
#Reading the exported csv
split_df <- split(read_clustered_nodetable$Ensembl,read_clustered_nodetable$gLayCluster)
#Splitting the node table by cluster
nodecount <- sapply(split_df, length)
#Counting how many nodes are in each cluster
countmatrix <- matrix(seq(1,length(nodecount)), ncol=1)
countmatrix <- cbind(countmatrix,as.numeric(nodecount))
#Construcing a matrix showing how many nodes are in each cluster
invalidclusters <- as.list(countmatrix[countmatrix[, 2] < 5, 1])
#Getting which clusters have fewer than 5 nodes associated with them
valid_clustered_nodetable <- read_clustered_nodetable[!read_clustered_nodetable$gLayCluster %in% invalidclusters, ]
#Generating a new df containing only nodes associated with clusters that had 5 or more nodes
split_tbl <- split(valid_clustered_nodetable, valid_clustered_nodetable$gLayCluster)
sourcecount <- function(cluster) {
wpcount <- sum(split_tbl[[cluster]][["fromWikiPathways"]] == 1, na.rm = TRUE)
dgcount <- sum(split_tbl[[cluster]][["fromDisGeNET"]] == 1, na.rm = TRUE)
litcount <- sum(split_tbl[[cluster]][["fromPublication"]] == 1, na.rm = TRUE)
stringcount <- sum(split_tbl[[cluster]][["fromSTRINGnode"]] == 1, na.rm = TRUE)
result_df <- data.frame(
gLayCluster = split_tbl[[cluster]][["gLayCluster"]][1],
WikiPathways_source = wpcount,
DisGeNET_source = dgcount,
Publication_source = litcount,
STRING_source = stringcount
)
}
sources_count <- do.call(rbind, lapply(seq_along(split_tbl),sourcecount))
#For each cluster, counting how many nodes are associated with which sources
cnvassociatedcount <- function(cluster) {
cnvcount <- sum(split_tbl[[cluster]][["CNVassociated"]] == 1,na.rm=TRUE)
nodecount <- length(split_tbl[[cluster]][["Ensembl"]])
result_df <- data.frame(
gLayCluster = split_tbl[[cluster]][["gLayCluster"]][1],
N_CNVassociated_nodes = cnvcount,
proportion_CNVassociated_nodes = round((cnvcount / nodecount)*100)
)
}
cnvassociated_count <- do.call(rbind,lapply(seq_along(split_tbl),cnvassociatedcount))
#For each cluster, count how many nodes originally come from CNV-associated pathways which pathways they come from
## GO ANALYSIS ------------------------------------------------------------------------------------------------------------------------
split_df <- split(valid_clustered_nodetable$Ensembl,valid_clustered_nodetable$gLayCluster)
split_list <- lapply(split_df, as.vector)
#Splitting the node table by cluster number, i.e. lists of Ensembl IDs are created per cluster
go <- function(cluster) {
gost(
query = cluster,
organism = "hsapiens",
ordered_query = FALSE,
multi_query = TRUE,
significant = TRUE,
exclude_iea = FALSE,
measure_underrepresentation = FALSE,
evcodes = FALSE,
user_threshold = 0.05,
correction_method = "g_SCS",
domain_scope ="annotated",
custom_bg = NULL,
numeric_ns = "",
sources = NULL,
as_short_link = FALSE,
highlight = TRUE
)
}
go_list <- lapply(split_list,go)
#Iterating the gost GO function over all clusters
saveRDS(go_list, file=paste0(nw_savepath,"/go_list.rds"))
#Saving the entire generated GO analysis as R object locally
get_top_terms <- function(cluster) {
terms <- toString(go_list[[cluster]][["result"]][["term_name"]][1:5])
#Extracting the top 5 term names associated with each cluster
pval <- toString(go_list[[cluster]][["result"]][["p_values"]][1:5])
#Extracting the p-values for the corresponding top 5 term names
nodes <- paste(go_list[[cluster]][["meta"]][["query_metadata"]][["queries"]][["query_1"]],collapse=",")
nnodes <- str_count(toString(go_list[[cluster]][["meta"]][["query_metadata"]][["queries"]][["query_1"]]),"\\S+")
#Extracing the number of nodes/genes contained in each cluster
result_df <- data.frame(
gLayCluster = cluster,
GO_Terms = terms,
GO_Pvals = pval,
Nodes = nodes,
N_nodes = nnodes
)
}
topterms_df <- do.call(rbind, lapply(names(go_list),get_top_terms))
#Getting top 5 term names and corresponding p-values for each cluster and storing in topterms_df
topterms_df <- cbind(topterms_df,sources_count,cnvassociated_count)
#joining the cluster table and the table detailing the amount of sources per cluster
write.table(topterms_df, file=paste0(getwd(),"/Data/GO-clusters-vis.tsv"), sep = "\t",row.names=FALSE,quote=FALSE)
#Writing the table to file for Cytoscape import during visualisation
loadTableData(topterms_df,data.key.column="gLayCluster",table.key.column="gLayCluster")
#Loading the generated top terms and p-values back to the supernetwork; every gene belonging to cluster x is now associated with the top terms of cluster x
deleteTableColumn('gLayCluster.1')
#Deleting duplicate gLayCluster column that appears after importing top terms data back to network
renameNetwork(title=paste0(getNetworkName(),"_GO"))
snw_scz_filtered_string_clustered_go <- getNetworkName()
#Renaming and saving the network name to indicate addition of GO information
compare_term_id_lists <- function(list1, list2) {
common_elements <- intersect(list1, list2)
return(length(common_elements))
}
#Setting up a function to get intersections between cluster term IDs
match_df <- data.frame(Cluster1 = character(),
Cluster2 = character(),
Matches = numeric(),
stringsAsFactors = FALSE)
#Setting up a df to store output in
for (i in 1:(length(go_list) - 1)) {
for (j in (i + 1):length(go_list)) {
term_id_i <- go_list[[i]][["result"]][["term_id"]]
term_id_j <- go_list[[j]][["result"]][["term_id"]]
matches <- compare_term_id_lists(term_id_i, term_id_j)
match_df <- rbind(match_df, data.frame(Cluster1 = names(go_list)[i],
Cluster2 = names(go_list)[j],
Matches = matches))
#Iterating over go_list to compare GO term IDs between every cluster and store number of overlaps
}
}
colnames(match_df) <- c("source","target","GO_term_matches")
#Renaming columns
write.table(match_df, file=paste0(getwd(),"/Data/match_df.tsv"), sep = "\t",row.names=FALSE,quote=FALSE)
exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW_filtered_STRING_clustered_GO"),"CX",network=snw_scz_filtered_string_clustered_go,overwriteFile=TRUE)
#Exporting the filtered, stringified, clustered supernetwork after GO as cx file and tagging it with the time and data to match with the metadata file
createColumnFilter(
filter.name = "has_GO_result",
column = "N_nodes",
criterion = 0,
predicate = "GREATER_THAN",
anyMatch = TRUE,
apply = TRUE
)
#Selecting nodes included in a 'valid' cluster, i.e. clusters with 5 or more nodes (GO analysis only performed for these)
#N_nodes is only generated for 'valid' clusters, so good column to filter by
invertNodeSelection()
deleteSelectedNodes()
sparqlquery("AOP-Wiki","KERKEquery.txt","KERList")
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
write.table(KERList, file=paste0(getwd(),"/Data/AOP-Wiki/KERList.tsv"),quote=FALSE,row.names=FALSE,sep="\t")
#Writing to file
sparqlquery("AOP-Wiki","KEtitlequery.txt","KEMap")
#Sending a query to AOP-Wiki to get KE and corresponding titles
write.table(KEMap,file=paste0(getwd(),"/Data/AOP-Wiki/KEMap.tsv"))
#Writing to file
sparqlquery("AOP-Wiki","KEensemblquery.txt","KEEnsembl")
#Sending a query to AOP-Wiki to get all KE-Ensembl ID pairings available
allKEsFromList <- data.frame(allKEs = c(KERList$KEup,KERList$KEdown))
#Combining both up- and downregulated KEs into a single column
filtered_KEEnsembl <- subset(KEEnsembl, KE %in% allKEsFromList$allKEs)
#Removing KE-Ensembl rows that are not found in the selected KEs
write.table(filtered_KEEnsembl, file=paste0(getwd(),"/Data/AOP-Wiki/KEEnsembl_filtered.tsv"), quote=FALSE,row.names=FALSE,sep="\t" )
#Writing the filtered KE-ENSG list to file for Cytoscape loading
View(KERList)
View(KEMap)
#Writing to file
KERList$KEuptitle <- []
#Writing to file
KERList$KEuptitle <- c()
View(KERList)
#Writing to file
KERList$KEuptitle <- NA
View(KERList)
#Writing to file
KERList$KERtitle <- NA
KERList$KEdowntitle <- NA
sparqlquery("AOP-Wiki","newAOPquery.txt","aoptest")
View(aoptest)
for (i in 1:ncol(aoptest)) {
for (j in 1:nrow(aoptest)) {
df[j, i] <- gsub('"', '', df[j, i])
}
}
for (i in 1:ncol(aoptest)) {
for (j in 1:nrow(aoptest)) {
aoptest[j, i] <- gsub('"', '', df[j, i])
}
}
for (i in 1:ncol(aoptest)) {
for (j in 1:nrow(aoptest)) {
aoptest[j, i] <- gsub('"', '', aoptest[j, i])
}
}
View(aoptest)
View(aoptest)
sparqlquery("AOP-Wiki","newAOPquery.txt","df")
for (i in 1:ncol(df)) {
for (j in 1:nrow(df)) {
df[j, i] <- gsub('"', '', df[j, i])
}
}
df_separated <- separate_rows(df, Ensembl, sep = ", ")
library(tidyr)
df_separated <- separate_rows(df, Ensembl, sep = ", ")
# Create a new DataFrame to store the result
result_df <- data.frame(KE = character(), Ensembl = character(), stringsAsFactors = FALSE)
# Iterate through each row of separated DataFrame
for (i in 1:nrow(df_separated)) {
# Split KE values by comma
kes <- unlist(strsplit(as.character(df_separated$KE[i]), ","))
# Create a new row for each KE value
for (ke in kes) {
result_df <- rbind(result_df, data.frame(KE = trimws(ke), Ensembl = df_separated$Ensembl[i], stringsAsFactors = FALSE))
}
}
View(result_df)
# Separate Ensembl column into multiple rows
df_separated <- separate_rows(df, Ensembl, sep = "/")
# Group by Ensembl and combine KE values
result_df <- df_separated %>%
group_by(Ensembl) %>%
summarise(KE = paste(KE, collapse = "/"))
View(result_df)
sparqlquery("AOP-Wiki","newAOPquery.txt","df")
for (i in 1:ncol(df)) {
for (j in 1:nrow(df)) {
df[j, i] <- gsub('"', '', df[j, i])
}
}
# Separate Ensembl column into multiple rows
df_separated <- separate_rows(df, Ensembl, sep = "/")
# Group by Ensembl and combine KE values
result_df <- df_separated %>%
group_by(Ensembl) %>%
summarise(KE = paste(KE, collapse = "/"))
View(result_df)
sparqlquery("AOP-Wiki","newAOPquery.txt","df")
for (i in 1:ncol(df)) {
for (j in 1:nrow(df)) {
df[j, i] <- gsub('"', '', df[j, i])
}
}
df <- separate_rows(df, KE, sep = ",")
# Group by Ensembl and concatenate KE values
result_df <- df %>%
group_by(Ensembl, KE) %>%
summarise() %>%
group_by(Ensembl) %>%
summarise(KE = paste(KE, collapse = ","))
View(result_df)
sparqlquery("AOP-Wiki","newAOPquery.txt","df")
for (i in 1:ncol(df)) {
for (j in 1:nrow(df)) {
df[j, i] <- gsub('"', '', df[j, i])
}
}
View(df)
new_df <- separate_rows[df,Ensembl,sep=", "]
new_df <- separate_rows(df,Ensembl,sep=", ")
View(new_df)
anewdf <- new_df %>%
group_by(Ensembl) %>%
summarise(KE = paste(KE, collapse=", "),
KEtitle = paste(KEtitle, collapse=", "),
AOP = paste(AOP, collapse=", "),
AO = paste(AO, collapse = ", "))
View(anewdf)
sparqlquery("AOP-Wiki","newAOPquery.txt","keensgpairs")
for (i in 1:ncol(keensgpairs)) {
for (j in 1:nrow(keensgpairs)) {
keensgpairs[j, i] <- gsub('"', '', keensgpairs[j, i])
}
}
separate_keensgpairs <- separate_rows(keensgpairs,Ensembl,sep=", ")
keensgpairs_byensg <- separate_keensgpairs %>%
group_by(Ensembl) %>%
summarise(KE = paste(KE, collapse=", "),
KEtitle = paste(KEtitle, collapse=", "),
AOP = paste(AOP, collapse=", "),
AO = paste(AO, collapse = ", "))
View(keensgpairs_byensg)
count(unique(keensgpairs_byensg$Ensembl))
length(unique(keensgpairs_byensg$Ensembl))
createColumnFilter(
filter.name = "has_GO_result",
column = "N_nodes",
criterion = 0,
predicate = "GREATER_THAN",
anyMatch = TRUE,
apply = TRUE
)
#Selecting nodes included in a 'valid' cluster, i.e. clusters with 5 or more nodes (GO analysis only performed for these)
#N_nodes is only generated for 'valid' clusters, so good column to filter by
invertNodeSelection()
deleteSelectedNodes()
#Inverting the selection and deleting these nodes: now, the network contains only the nodes that make up the clusters fed into the GO analysis
#Nodes not associated to a large enough cluster/GO term are likely not involved in any significant SCZ-contributing way
#The idea is to link GO terms (formed by clusters/genes) to risk factors, so it wouldn't make sense to also link non-cluster/GO associated nodes
sparqlquery("AOP-Wiki","newAOPquery.txt","keensgpairs")
for (i in 1:ncol(keensgpairs)) {
for (j in 1:nrow(keensgpairs)) {
keensgpairs[j, i] <- gsub('"', '', keensgpairs[j, i])
}
}
#Removing quotation marks from the df
separate_keensgpairs <- separate_rows(keensgpairs,Ensembl,sep=", ")
keensgpairs_byensg <- separate_keensgpairs %>%
group_by(Ensembl) %>%
summarise(KE = paste(KE, collapse=", "),
KEtitle = paste(KEtitle, collapse=", "),
AOP = paste(AOP, collapse=", "),
AO = paste(AO, collapse = ", "))
write.table(file=paste0(getwd(),"/Data/AOP-Wiki/keensgpairs_byensg.tsv"),quote=FALSE, sep="\t", row.names=FALSE)
write.table(keensgpairs_byensg, file=paste0(getwd(),"/Data/AOP-Wiki/keensgpairs_byensg.tsv"),quote=FALSE, sep="\t", row.names=FALSE)
createColumnFilter(
filter.name = "has_GO_result",
column = "N_nodes",
criterion = 0,
predicate = "GREATER_THAN",
anyMatch = TRUE,
apply = TRUE
)
#Selecting nodes included in a 'valid' cluster, i.e. clusters with 5 or more nodes (GO analysis only performed for these)
#N_nodes is only generated for 'valid' clusters, so good column to filter by
invertNodeSelection()
deleteSelectedNodes()
keensgpairs_byensg_save <- paste0(getwd(),"/Data/AOP-Wiki/keensgpairs_byensg.tsv")
keensgpairs_byensg_save <- paste0(getwd(),"/Data/AOP-Wiki/keensgpairs_byensg.tsv")
write.table(keensgpairs_byensg, file=keensgpairs_byensg_save,quote=FALSE, sep="\t", row.names=FALSE)
commandsRun(sprintf('table import file dataTypeTargetforNetworkCollection="Node Table Columns" delimiters=\\t file=%s firstRowAsColumnNames=true keyColumnForMapping="Ensembl" keyColumnIndex=5 startLoadRow=1'),keensgpairs_byensg_save)
commandsRun(sprintf('table import file dataTypeTargetforNetworkCollection="Node Table Columns" delimiters=\\t file=%s firstRowAsColumnNames=true keyColumnForMapping="Ensembl" keyColumnIndex=5 startLoadRow=1',keensgpairs_byensg_save))
commandsRun(sprintf('table import file dataTypeTargetforNetworkList"Node Table Columns" delimiters=\\t file=%s firstRowAsColumnNames=true keyColumnForMapping="Ensembl" keyColumnIndex=5 startLoadRow=1',keensgpairs_byensg_save))
commandsRun(sprintf('table import file dataTypeTargetforNetworkList="Node Table Columns" delimiters=\\t file=%s firstRowAsColumnNames=true keyColumnForMapping="Ensembl" keyColumnIndex=5 startLoadRow=1',keensgpairs_byensg_save))
commandsRun(sprintf('table import file dataTypeTargetforNetworkCollection="Node Table Columns" delimiters=\\t file=%s firstRowAsColumnNames=true keyColumnForMapping="Ensembl" keyColumnIndex=1 startLoadRow=1',keensgpairs_byensg_save))
sparqlquery("AOP-Wiki","newAOPquery.txt","keensgpairs")
View(keensgpairs)
createColumnFilter(
filter.name = "has_GO_result",
column = "N_nodes",
criterion = 0,
predicate = "GREATER_THAN",
anyMatch = TRUE,
apply = TRUE
)
#Selecting nodes included in a 'valid' cluster, i.e. clusters with 5 or more nodes (GO analysis only performed for these)
#N_nodes is only generated for 'valid' clusters, so good column to filter by
invertNodeSelection()
deleteSelectedNodes()
#Inverting the selection and deleting these nodes: now, the network contains only the nodes that make up the clusters fed into the GO analysis
#Nodes not associated to a large enough cluster/GO term are likely not involved in any significant SCZ-contributing way
#The idea is to link GO terms (formed by clusters/genes) to risk factors, so it wouldn't make sense to also link non-cluster/GO associated nodes
sparqlquery("AOP-Wiki","newAOPquery.txt","keensgpairs")
for (i in 1:ncol(keensgpairs)) {
for (j in 1:nrow(keensgpairs)) {
keensgpairs[j, i] <- gsub('"', '', keensgpairs[j, i])
}
}
#Removing quotation marks from the df
separate_keensgpairs <- separate_rows(keensgpairs,Ensembl,sep="; ")
keensgpairs_byensg <- separate_keensgpairs %>%
group_by(Ensembl) %>%
summarise(KE = paste(KE, collapse="; "),
KEtitle = paste(KEtitle, collapse="; "),
AOP = paste(AOP, collapse="; "),
AO = paste(AO, collapse = "; "))
View(keensgpairs)
keensgpairs_byensg <- separate_keensgpairs %>%
group_by(Ensembl) %>%
summarise(KEid = paste(KEid, collapse="; "),
KEtitle = paste(KEtitle, collapse="; "),
AOid = paste(AOid, collapse="; "),
AOtitle = paste(AOtitle, collapse = "; "),
AOPid = paste(AOPid, collapse="; "),
AOPtitle =paste(AOPtitle, collapse="; "))
keensgpairs_byensg_save <- paste0(getwd(),"/Data/AOP-Wiki/keensgpairs_byensg.tsv")
write.table(keensgpairs_byensg, file=keensgpairs_byensg_save,quote=FALSE, sep="\t", row.names=FALSE)
commandsRun(sprintf('table import file dataTypeTargetforNetworkCollection="Node Table Columns" delimiters=\\t file=%s firstRowAsColumnNames=true keyColumnForMapping="Ensembl" keyColumnIndex=1 startLoadRow=1',keensgpairs_byensg_save))
