#Running query to get AOPid-title mappings
for (i in 1:ncol(aopmap)) {
for (j in 1:nrow(aopmap)) {
aopmap[j, i] <- gsub('"', '', aopmap[j, i])
}
}
#Removing quotation marks from df
keaoppairs <- merge(keaoppairs,aopmap,by="AOPid",all.x=FALSE)
#Mapping AOPids to AOPtitles using mapping file
keaoppairs <- keaoppairs %>%
rename(KEid_target = KEid,
AOPid_source = AOPid)
#Renaming columns in preparation for import
keaoppairs$KEid <- keaoppairs$KEid_target
keaoppairs$AOPid <- keaoppairs$AOPid_source
#Creating duplicate columns of KEid and AOPid to be used as source and target attributes
#This allows new columns in the network to easily select AOP and KE nodes separately etc.
#Without this, both KE and AOP nodes are stored in the 'names' column due to how Cytoscape import works
keaoppairs <- keaoppairs[,c("KEid_target","KEid","AOPid_source","AOPid","AOPtitle")]
#Reordering columns
write.table(keaoppairs,file=paste0(other_savepath,sprintf("/keaoppairs%s.tsv",sub("top","",deparse(substitute(input))))),sep="\t",quote=FALSE,row.names=FALSE)
#Writing modified table to file
commandsRun(sprintf('network import file columnTypeList=t,ta,s,sa,sa delimiters=\\t file=%s firstRowAsColumnNames=true rootNetworkList= -- Create new network collection -- startLoadRow=1',paste0(other_savepath,sprintf("/keaoppairs%s.tsv",sub("top","",deparse(substitute(input)))))))
#Importing as network
Sys.sleep(0.5)
renameNetwork(sprintf("Top quarter key events - AOPs%s",sub("top","",deparse(substitute(input)))))
}
# getkeaoppairs(top_selected)
# keaopnetwork_selected <- getNetworkName()
getkeaoppairs(top_all)
keaopnetwork_all <- getNetworkName()
getaopaopairs <- function(input) {
keaoppairs <- keensgpairs[keensgpairs$KEid %in% input$KEid,"AOPid"]
keaoppairs <- unique(separate_rows(keaoppairs,AOPid,sep="; "))
#For top quarter KEs, get which unique AOPs these are taken from
sparqlquery("AOP-Wiki","aopao.txt","aopao")
#Getting full list of which AOs are associated to which AOPs
aopaopairs <- aopao[aopao$AOPid %in% keaoppairs$AOPid,]
#Filtering AOP-AO list by AOPs associated with top quarter KEs
sparqlquery("AOP-Wiki","aopmap.txt","aopmap")
#Running query to get AOPid-title mappings
for (i in 1:ncol(aopmap)) {
for (j in 1:nrow(aopmap)) {
aopmap[j, i] <- gsub('"', '', aopmap[j, i])
}
}
#Removing quotation marks from df
aopaopairs <- merge(aopaopairs,aopmap,by="AOPid",all.x=FALSE)
#Mapping AOPids to AOPtitles using mapping file
sparqlquery("AOP-Wiki","aomap.txt","aomap")
for (i in 1:ncol(aomap)) {
for (j in 1:nrow(aomap)) {
aomap[j, i] <- gsub('"', '', aomap[j, i])
}
}
#Removing quotation marks from df
aopaopairs <- merge(aopaopairs,aomap,by="AOid",all.x=FALSE)
#Mapping AOids to AOtitles using mapping file
aopaopairs <- aopaopairs %>%
rename(AOPid_target = AOPid,
AOid_source = AOid)
#Renaming columns in preparation for import
aopaopairs$AOid <- aopaopairs$AOid_source
aopaopairs$AOPid <- aopaopairs$AOPid_target
#Creating duplicate columns of AOid and AOPid to be used as source and target attributes
#This allows new columns in the network to easily select AOid and AOPid nodes separately etc.
#Without this, both AO and AOP nodes are stored in the 'names' column due to how Cytoscape import works
aopaopairs$index <- NA
#Creating an empty placeholder column that is to be filled with copies of row values from the SUID column; explained more in mergeaop function later
#Must be created in df that then becomes a network, since creating a column using the Cytoscape command line will result in an 'invisible' column that cannot be used as key for merging tables
aopaopairs <- aopaopairs[,c("AOid_source","AOPid_target","AOid","AOtitle","AOPid","AOPtitle","index")]
#Reordering columns
write.table(aopaopairs,file=paste0(other_savepath,sprintf("/aopaopairs%s.tsv",sub("top","",deparse(substitute(input))))),sep="\t",quote=FALSE,row.names=FALSE)
#Writing modified table to file
commandsRun(sprintf('network import file columnTypeList=s,t,sa,sa,ta,ta,ta delimiters=\\t file=%s firstRowAsColumnNames=true rootNetworkList= -- Create new network collection -- startLoadRow=1',paste0(other_savepath,sprintf("/aopaopairs%s.tsv",sub("top","",deparse(substitute(input)))))))
#Importing table as network
Sys.sleep(0.5)
renameNetwork(sprintf("AOP-AO pairs for top AOPs in top quarter KEs%s",sub("top","",deparse(substitute(input)))))
}
# getaopaopairs(top_selected)
# aopaonetwork_selected <- getNetworkName()
getaopaopairs(top_all)
aopaonetwork_all <- getNetworkName()
mergeaop <- function (input){
aopaonetwork <- get(paste0("aopaonetwork_",input))
keaopnetwork <- get(paste0("keaopnetwork_", input))
kegenenetwork <- get(paste0("kegenenetwork_",input))
#Renaming variables to contain type of selection to correctly select previously generated objects
altmergeNetworks(sources=c(aopaonetwork,keaopnetwork),
title = "KE-AOP-AO merged network",
operation="union",
nodeKeys=c("AOPid","AOPid"))
renameNetwork(paste0("KE-AOP-AO merged network_",input))
#Merging the AOP-AO network to the KE-AOP network to extend KE-AOP associations with AOs
keaopaomerged <- getNetworkName()
altmergeNetworks(sources=c(keaopaomerged,kegenenetwork),
title="gene-KE-AOP-AO merged network",
operation="union",
nodeKeys=c("KEid","KEid"))
#Merging KE-AOP-AO associations with the KE-gene network to extend KE-gene associations with AOPs and AOs
renameNetwork(paste0("gene-KE-AOP-AO merged network_",input))
mapTableColumn("Ensembl","Human","Ensembl","HGNC")
#Generating HGNC names for gene nodes to improve readability
commandsRun(sprintf('table set values columnName=index handleEquations=true rowList=all value="=$SUID" table=%s',paste0("gene-KE-AOP-AO merged network_",input," default  node")))
#Filling the  index column with node SUIDs
#SUIDs are always generated for each node but are not exported with the table, so they must first be transposed to a column that will be exported
commandsRun(sprintf('table export options=CSV outputFile="%s" table="%s"',paste0(other_savepath,paste0("gene-KE-AOP-AO merged network_",input," node")),paste0("gene-KE-AOP-AO merged network_",input," default  node")))
#Exporting the node table of the current network
nodetable <- read.table(paste0(other_savepath,paste0('gene-KE-AOP-AO merged network_',input," node.csv")), header=TRUE, sep=",")
#Reading the node table as R object
nodetable <- nodetable %>%
rowwise() %>%
mutate(label=paste(na.omit(c_across(all_of(c("KEtitle","AOPtitle","AOtitle","HGNC")))), collapse=""))
#Generating a new 'label' column that combines KEtitle, AOPtitle, AOtitle, and HGNC into one columns
#Since each node represents a different type of data (KE, AOP, AO, or gene), titles will never overlap
#Cytoscape visualisation is based on one column, thus labels need to all be stored in a single column for visualisation
nodetable <- nodetable %>%
rowwise() %>%
mutate(type = case_when(
str_detect(AOPid, "\\S") ~ "AOP",
str_detect(AOid, "\\S") ~ "AO",
str_detect(KEid, "\\S") ~ "KE",
str_detect(Ensembl, "\\S") ~ "gene",
TRUE ~ NA_character_
))
#Generating a new 'type' column indicating of which type (KE, AOP, AO, or gene) a node is
#Again for visualsation purposes - later used to determine color mapping of node based on type
write.table(nodetable, file=paste0(other_savepath,paste0("gene-KE-AOP-AO merged network_",input," node.tsv")),sep="\t",row.names = FALSE,quote = FALSE)
commandsRun(sprintf('table import file file=%s dataTypeTargetForNetworkCollection="Node Table Columns" delimiters=\\t keyColumnForMapping="index"  keyColumnIndex=7 startLoadRow=1 firstRowAsColumnNames=true',paste0(other_savepath,paste0("gene-KE-AOP-AO merged network_",input," node.tsv"))))
#Loading modified node table back to the network to include label and type columns
#In AOP-Wiki, KEs and AOs have the same type of URL, and some data may be considered an AO in one AOP and a KE in another
#The only column where every node has a value is 'name', but since the same URL (used for 'name') may designate two different types (AO or KE),
#using 'name' as key column for table reimport causes nodes with the same 'name' to be merged, even though they are different
#Therefore, SUID is used as key column as it is unique for every node
#Now, a datapoint that represents an AO in one AOP and a KE in another AOP is kept as two distinct nodes as desired
deleteTableColumn("shared.name")
#Deleting duplicate column
lapply(c(aopaonetwork,keaopnetwork,kegenenetwork,keaopaomerged),deleteNetwork)
#Deleting intermediary networks used to generate full gene-KE-AOP-AO network
}
mergeaop("all")
#Creating the full gene-KE-AOP-AO network with data from all AOs and associated AOPs and KEs in AOP-Wiki
exportNetwork(paste0(nw_savepath,"gene-KE-AOP-AO merged network_all"),"CX", overwriteFile=TRUE,network="gene-KE-AOP-AO merged network_all")
#Exporting the network
#mergeaop("selected")
#exportNetwork(paste0(nw_savepath,"gene-KE-AOP-AO merged network_selected"),"CX", overwriteFile=TRUE,network="gene-KE-AOP-AO merged network_selected")
end_section("AOP")
importNetworkFromFile(paste0(nw_savepath,"SCZ_SNW_STRING_clustered_GO.cx"))
#Reimporting clustered supernetwork with GO results added
commandsRun('table delete column column="gLayCluster.2" table="SCZ_SNW_STRING_clustered_GO default node')
#Deleting duplicate gLayCluster column
createColumnFilter(
filter.name = "has_GO_result",
column = "N_nodes",
criterion = 0,
predicate = "GREATER_THAN",
anyMatch = TRUE,
apply = TRUE
)
#Selecting nodes included in a 'valid' cluster, i.e. clusters with 5 or more nodes (GO analysis only performed for these)
#N_nodes is only generated for 'valid' clusters, so good column to filter by
invertNodeSelection()
deleteSelectedNodes()
#Inverting selection and deleting nodes that don't have GO results
commandsRun(sprintf('table export options=CSV outputFile=%s table="SCZ_SNW_STRING_clustered_GO default node"',paste0(other_savepath,"SCZ_SNW_STRING_clustered_GO node")))
#Exporting the node table from the clustered supernetwork with GO results
snw_node <- read.table(file=paste0(other_savepath,"SCZ_SNW_STRING_clustered_GO node.tsv"),header=TRUE,sep="\t")
#Exporting the node table from the clustered supernetwork with GO results
snw_node <- read.table(file=paste0(other_savepath,"SCZ_SNW_STRING_clustered_GO node.csv"),header=TRUE,sep=",")
deleteNetwork("SCZ_SNW_STRING_clustered_GO")
aopmerged_node <- read.table(file=paste0(other_savepath,"gene-KE-AOP-AO merged network_all node.tsv"),header=TRUE,sep="\t")
snw_node_aop <- snw_node[snw_node$Ensembl %in% aopmerged_node$Ensembl,]
remove_duplicates <- function(pathway_string) {
pathway_string %>%
str_split(";|, ") %>%     # Split the string by commas or semicolons
unlist() %>%              # Unlist the resulting list
unique() %>%              # Remove duplicates
paste(collapse = ", ")    # Collapse the unique elements back into a single string
}
#When merging WikiPathways node and edge networks, it's possible that there are semi-colon separated duplicates in PathwayID
#This function splits strings and removes duplicates for a more consistent and clean look
snw_node_aop$PathwayID <- sapply(snw_node_aop$PathwayID, remove_duplicates)
snw_node_aop_valid <- select(snw_node_aop,Ensembl, fromPublication,fromSTRING,fromWikiPathways,fromDisGeNET,gLayCluster,snpID,CNVassociated,PathwayID)
#Getting relevant columns for network construction
snw_node_aop_valid <- separate_rows(snw_node_aop_valid,PathwayID,sep=", ")
#Separating PathwayID rows to have one or multiple PathwayIDs per gene; one PathwayID per row
sparqlquery("WikiPathways","pathwaymap.txt","pathwaymap")
#Querying WikiPathways for all Pathway IDs and Pathway titles for mapping
pathwaymap$PathwayTitle <- sub("@en$","",pathwaymap$PathwayTitle)
pathwaymap$PathwayTitle <- gsub('"','',pathwaymap$PathwayTitle)
#Cleaning up mapping file by removing quotation marks and "@en" appendix of all rows
snw_node_aop_valid <- merge(snw_node_aop_valid,pathwaymap,by="PathwayID",all.x=TRUE)
View(snw_node_aop_valid)
?BridgeDbR
#Adding PathwayTitle to the df to map pathway IDs to titles
bridgedb_hs <- getDatabase("Homo sapiens")
?BridgeDbR
getwd()
bridgedb_hs <- getDatabase("Homo sapiens",location=paste0(getwd(),"/BridgeDb"))
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
download_confirm <- function(bridgedb_dir, confirmation = "Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(bridgedb_dir)) {
message("File already present at ", bridgedb_dir)
} else {
confirm <- readLine(promt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=bridgedb_dir)
message("Downloading mapping file to ",bridgedb_dir)
} else {
message("File download cancelled.")
}
}
}
download_confirm()
#Defining directory in which BridgeDb mapping file is stored
download_confirm <- function(bridgedb_dir = bridgedb_dir, confirmation = "Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(bridgedb_dir)) {
message("File already present at ", bridgedb_dir)
} else {
confirm <- readLine(promt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=bridgedb_dir)
message("Downloading mapping file to ",bridgedb_dir)
} else {
message("File download cancelled.")
}
}
}
download_confirm()
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
download_confirm <- function(dir = bridgedb_dir, confirmation = "Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir)
} else {
confirm <- readLine(promt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=dir)
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
download_confirm()
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
download_confirm <- function(dir = bridgedb_dir, confirmation = "Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, ". No files downloaded.")
} else {
confirm <- readLine(promt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=dir)
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
download_confirm()
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
download_confirm <- function(dir = bridgedb_dir, confirmation = "Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readLine(promt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=dir)
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
download_confirm()
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readLine(promt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=dir)
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readLine(promt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=dir)
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(promt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=dir)
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=dir)
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "BridgeDb file not detected. Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=dir)
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
bridgedb_dir <- paste0(getwd(),"/BridgeDb/")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "BridgeDb file not detected. Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=dir)
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "BridgeDb file not detected. Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=paste0(getwd(),"/BridgeDb"))
message("Downloading mapping file to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "BridgeDb mapping file not detected. Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=paste0(getwd(),"/BridgeDb"))
message("BridgeDb mapping file downloaded to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "BridgeDb mapping file not detected. Download BridgeDb mapping file for Homo sapiens? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=paste0(getwd(),"/BridgeDb"))
message("BridgeDb mapping file downloaded to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
?BridgeDb
?BridgeDbR
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_dir)
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "BridgeDb mapping file not detected. Download BridgeDb mapping file for Homo sapiens (800.4 MB)? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <<- getDatabase("Homo sapiens",location=paste0(getwd(),"/BridgeDb"))
message("BridgeDb mapping file downloaded to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
bridgedb_hs <- getDatabase("Homo sapiens",location=paste0(getwd(),"/BridgeDb"))
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
?RJavaTools
library(RJavaTools)
install.packages("RJavaTools")
options(java.parameters = "-Xmx1000m")
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
bridgedb_hs
?rJava
library(rJava)
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
?rJava
install.packages("rJava")
bridgedb_dir <- paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge")
#Defining directory in which BridgeDb mapping file is stored
getBridgeDbmap <- function(dir = bridgedb_dir, confirmation = "BridgeDb mapping file not detected. Download BridgeDb mapping file for Homo sapiens (800.4 MB)? (yes/no): ") {
if(file.exists(dir)) {
message("File already present at ", dir, " No files downloaded.")
} else {
confirm <- readline(prompt = confirmation)
if (tolower(confirm) == "yes") {
bridgedb_hs <- getDatabase("Homo sapiens",location=paste0(getwd(),"/BridgeDb"))
message("BridgeDb mapping file downloaded to ",dir)
} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
?`rJava-internal`
?.jinit
#Downloading the BridgeDb bridge file for Homo sapiens identifiers to the repo
#The directory is added to .gitignore to avoid uploading it to GitHub
#This is a relatively large (800MB) file - downloading it once is sufficient
.jinit(parameters=c(sprintf("-Djava.class.path=%s",paste0(getwd(),"/BridgeDb/derby.jar"))))
javaVersion <- .jcall("java/lang/System", "Ljava/lang/String;", "getProperty", "java.version")
print(javaVersion)
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
#Downloading the BridgeDb bridge file for Homo sapiens identifiers to the repo
#The directory is added to .gitignore to avoid uploading it to GitHub
#This is a relatively large (800MB) file - downloading it once is sufficient
.jinit(parameters=c(sprintf("-Djava.class.path=%s",paste0(getwd(),"/BridgeDb/derby.jar"))))
#A derby file needs to be intialized to use rJava on which BridgeDbR depends
.jaddClassPath(paste0(getwd(),"/BridgeDb/derby.jar"))
driver_class <- "org.apache.derby.jdbc.EmbeddedDriver"
.jclassLoader()$loadClass(driver_class)
?rJava
?`rJava-internal`
#Downloading the BridgeDb bridge file for Homo sapiens identifiers to the repo
#The directory is added to .gitignore to avoid uploading it to GitHub
#This is a relatively large (800MB) file - downloading it once is sufficient
.jinit(parameters=c(sprintf("-Djava.class.path=%s",paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge"))))
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
#A derby file needs to be intialized to use rJava on which BridgeDbR depends
.jaddClassPath(paste0(getwd(),"/BridgeDb/Hs_Derby_Ensembl_108.bridge"))
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
?curl
?`rJava-internal`
library(curl)
#Adding PathwayTitle to the df to map pathway IDs to titles
mapper <- loadDatabase(bridgedb_hs)
