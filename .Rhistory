} else {
message("File download cancelled.")
}
}
}
getBridgeDbmap()
#Downloading the BridgeDb bridge file for Homo sapiens identifiers to the repo
#The directory is added to .gitignore to avoid uploading it to GitHub
#This is a relatively large (800MB) file - downloading it once is sufficient
loadDatabase(bridgedb_dir)
#Loading the database
# FUNCTION DICTIONARY-------------------------------------------------------------------------------------------------------------------
.defaultBaseUrl <- 'http://127.0.0.1:1234/v1'
#Defining the default base URL found in the RCy3 source as R object for altmergeNetworks
altmergeNetworks <- function(               sources = NULL,
title = NULL,
operation = "union",
nodeKeys = NULL,
nodeMergeMap = NULL,
nodesOnly = FALSE,
edgeKeys = NULL,
edgeMergeMap = NULL,
networkMergeMap = NULL,
inNetworkMerge = TRUE,
base.url = .defaultBaseUrl) {
cmd.string <- 'network merge' # a good start
# sources must be suppled
if(is.null(sources)) {
message("Missing sources!")
return(NULL)
} else {
sources.str <- paste(sources, collapse = ",")
cmd.string <- paste0(cmd.string,' sources="',sources.str,'"')
}
# defaults
cmd.string <- paste0(cmd.string,' operation=',operation)
cmd.string <- paste0(cmd.string,' nodesOnly=',nodesOnly)
cmd.string <- paste0(cmd.string,' inNetworkMerge=',inNetworkMerge)
# optional args
if(!is.null(title))
cmd.string <- paste0(cmd.string,' netName="',title,'"')
if(!is.null(nodeKeys))
cmd.string <- paste0(cmd.string,' nodeKeys="',paste(nodeKeys, collapse = ","),'"')
if(!is.null(edgeKeys))
cmd.string <- paste0(cmd.string,' edgeKeys="',paste(edgeKeys, collapse = ","),'"')
if(!is.null(nodeMergeMap)){
nodeMergeMap.str <- paste(nodeMergeMap, collapse = ",")
nodeMergeMap.str <- gsub("c\\(", "{", nodeMergeMap.str)
nodeMergeMap.str <- gsub("\\)", "}", nodeMergeMap.str)
cmd.string <- paste0(cmd.string,' nodeMergeMap="',nodeMergeMap.str,'"')
}
if(!is.null(edgeMergeMap)){
edgeMergeMap.str <- paste(edgeMergeMap, collapse = ",")
edgeMergeMap.str <- gsub("c\\(", "{", edgeMergeMap.str)
edgeMergeMap.str <- gsub("\\)", "}", edgeMergeMap.str)
cmd.string <- paste0(cmd.string,' edgeMergeMap="',edgeMergeMap.str,'"')
}
if(!is.null(networkMergeMap)){
networkMergeMap.str <- paste(networkMergeMap, collapse = ",")
networkMergeMap.str <- gsub("c\\(", "{", networkMergeMap.str)
networkMergeMap.str <- gsub("\\)", "}", networkMergeMap.str)
cmd.string <- paste0(cmd.string,' networkMergeMap="',networkMergeMap.str,'"')
}
res.data <- commandsPOST(cmd.string, base.url = base.url)
if(!is.null(res.data$SUID))
return(res.data$SUID)
else
return(res.data)
}
#Normally, RCy3's 'mergeNetworks' function would be used to unify imported networks into one supernetwork
#This function does however not work on the latest RCy3 release (v.2.22.1), but does work when running the script on RCy3 v.2.14.2
#RCy3 2.14.2 requires R v.4.1.3, requiring the entire script to run on an old version of R for one function that is used once
#Here, we redefine the function using the source code from RCy3 v.2.14.2 and simply use this alternate function to merge networks
sparqlquery <- function(endpoint,queryfile,output) {
if (tolower(endpoint) %in% c("WikiPathways","wikipathways","wp","WP")) {
file_path <- paste0(getwd(),sprintf("/Data/WikiPathways/%s",queryfile))
#Specifying file path to the .txt file containing the query
base_url <- "https://sparql.wikipathways.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else if (tolower(endpoint) %in% c("AOP-Wiki", "aopwiki","aop-wiki","AOPWiki")) {
file_path <- paste0(getwd(),sprintf("/Data/AOP-Wiki/%s",queryfile))
#Specifying file path to the .txt file containing the query
base_url <- "https://aopwiki.rdf.bigcat-bioinformatics.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else if (tolower(endpoint) %in% c("DisGeNET","DGN","disgenet","Disgenet")) {
file_path <- paste0(getwd(),sprintf("/Data/DisGeNET/%s",queryfile))
#Specifying path to the .txt file containing the query
base_url <- "http://rdf.disgenet.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else
print("Error: Please specify WikiPathways, DisGeNET or AOP-Wiki as endpoint.")
querybody <- paste(readLines(file_path), collapse = "")
#Reading a text file containing a SPARQL query stored in the repo
encoded_query <- URLencode(querybody)
#Encoding the query as URL
full_url <- paste0(base_url, encoded_query, "&format=text/html&timeout=0&signal_void=on")
#Joining the query from the text file and the base URL and adding that output is desired as HTML
html <- read_html(full_url)
#Sending the query to the SPARQL endpoint and extracting as HTML
if(tolower(endpoint) %in% c("DisGeNET","DGN","disgenet","Disgenet")) {
tablebody <- html %>%
html_element("body") %>%
html_element("table") }
#DisGeNET has a slightly different HTML structure than AOP-Wiki and WikiPathways, so navigating to the table is done accordingly
else
tablebody <- html %>%
html_element("body") %>%
html_element("div") %>%
html_element("table")
#Navigating to the table output by the SPARQL query
assign(output, html_table(tablebody), envir=.GlobalEnv)
#Getting the output as tibble
}
#Function to send a SPARQL query defined in a local text file to the endpoint and extract to desired dataframe
#Will not work if query includes comments ('# this is a comment') due to HTML conversion
createNodeSource <- function(source,doi=NULL) {
if (source == "fromDisGeNET") {
networkname <- getNetworkName()
nodetable <- paste0(networkname," default  node")
}
#Networks imported from WikiPathways have a type in the node table designations, as they have 2 spaces between "default" and "node" instead of one
#This check determines which node table name format is to be applied depending on the source (WikiPathways or other)
else {
networkname <- getNetworkName()
nodetable <- paste0(networkname," default node")
}
commandsRun(sprintf("table create column columnName=fromWikiPathways table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromDisGeNET table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromPublication table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=Publication.doi table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromSTRING table=%s type=string",nodetable))
#Creating a new column for each source used for all networks
if ( source == "STRINGnode") {
commandsRun(sprintf('table set values columnName=%1$s handleEquations=false rowList="selected:true" table=%2$s value=1',source,nodetable))
}
else {
commandsRun(sprintf("table set values columnName=%1$s handleEquations=false rowList=all table=%2$s value=1",source,nodetable))
#Filling the new column of the corresponding source with 1 to indicate which source the node is imported from
}
if (!is.null(doi)) {
commandsRun(sprintf("table set values columnName=Publication.doi handleEquations=false rowList=all table=%1$s value=%2$s",nodetable,doi))
#Adding doi for literature used if provided
}
}
#Function to create new column in node table specifying origin of network/node
# SCHIZOPHRENIA =======================================================================================================================
## IMPORTING AND MERGING ---------------------------------------------------------------------------------------------------------------
start_section("Importing and merging")
sparqlquery("wp","metadataquery.txt","WikiPathways-SPARQL-metadata")
#Getting the metadata of the endpoint used for the WikiPathways SPARQL queries
metadata.add("WikiPathways SPARQL endpoint metadata")
metadata.add(`WikiPathways-SPARQL-metadata`)
#Adding the fetched metadata to the metadata file for the session
#It is technically possible that the metadata would describe an earlier version of the RDF if it is updated while the script runs but this is unlikely
sparqlquery("wp","pathwayquery.txt","wp_pathwaylist")
#Getting a list of pathways corresponding to a keyword as per defined in the query
writeLines(wp_pathwaylist[["PWID"]], con="Data/WikiPathways/Pathwaylists/automaticpathways.txt")
#Saving the output pathway list to file
manualpathways <- readLines("Data/WikiPathways/Pathwaylists/manualpathways.txt")
#Reading a file containing a list of manually selected pathways
allpathways <- c(wp_pathwaylist[["PWID"]],manualpathways)
#Joining the list of manually and automatically selected pathways together
allpathways_URL <- paste0("<",allpathways,">")
#Adding <> around all entries for easier use in SPARQL queries
#URLs need to be surrounded by <> to be recognised as such
writeLines(allpathways_URL, con="Data/WikiPathways/Pathwaylists/allpathways.txt")
#Writing list of all pathways in SPARQL URL format to file
sparqlquery("DisGeNET","disgenet-query.txt","gda")
#Querying the DisGeNET SPARQL endpoint for genes associated to schizophrenia from curated sources
gda$disgenet_curated <- str_extract(gda$source, "(?<=/)[^/]*$")
gda$Entrez_gene <- str_extract(gda$gene, "(?<=/)[^/]*$")
gda$HGNC_symbol <- str_extract(gda$symbol, "(?<=/)[^/]*$")
gda <- subset(gda, select=c(disgenet_curated,Entrez_gene,HGNC_symbol,gdascore))
#cleaning data
gda <- gda %>%
group_by(Entrez_gene, HGNC_symbol, gdascore) %>%
mutate(disgenet_curated = paste(disgenet_curated, collapse = "; ")) %>%
distinct()
gda <- gda[!duplicated(gda$Entrez_gene),]
#Concatenating to avoid duplicate gene rows if they are confirmed by multiple sources
mapper <- loadDatabase(bridgedb_dir)
#Loading bridgedb database
input <- data.frame(
source = rep("H", length(gda[, 3])),
identifier = gda[, 3]
)
#Making a new df to be used as input for bridgedb
#Map HGNC symbol
input <- input %>%
rename(source=source,
identifier=HGNC_symbol)
#Renaming cols for maps function compability
gda_map <- maps(mapper,input,"En")
#Mapping from HGNC to Ensembl
gda <- merge(gda,gda_map,by.x="HGNC_symbol",by.y="identifier",all.x=TRUE)
#Merging the GDA and mapping tables; some HGNC symbols can't be matched to Ensembl IDs but are still retained
gda <- subset(gda, select=c(HGNC_symbol,mapping,Entrez_gene,disgenet_curated,gdascore))
#Cleaning df
gda <- gda %>%
rename(Ensembl=mapping)
man_map <- read.delim(paste0(getwd(),"/Data/DisGeNET/manual-maps.txt"),sep="\t")
#Reading a file containing manual mappings for some of the missing Ensembl ID
gda <- merge(gda, man_map, by="HGNC_symbol",all.x=TRUE)
#Merging the manual map and the gda df based on HGNC symbol as key column
gda$Ensembl.x[is.na(gda$Ensembl.x)] <- gda$Ensembl.y[is.na(gda$Ensembl.x)]
#Merging the Ensembl cols
gda <- select(gda, -Ensembl.y)
#Removing superfluous Ensembl col from manual mapping df
gda <- gda %>%
rename(Ensembl = Ensembl.x)
gda$HGNC_symbol_source <- gda$HGNC_symbol
#Creating a duplicated Ensembl column for Cytoscape import
#Cleaning and renaming df
gda <- mutate_all(gda, ~ifelse(is.na(.),"",.))
#Replacing NA with empty strings for Cytoscape compatibility
write.table(gda,file=paste0(getwd(),"/Data/DisGeNET/gda.tsv"),quote=FALSE,sep="\t",row.names=FALSE)
commandsRun(sprintf('network import file columnTypeList=sa,sa,sa,sa,sa,s delimiters=\\t file=%s firstRowAsColumnNames=true startLoadRow=1',paste0(getwd(),"/Data/DisGeNET/gda.tsv")))
Sys.sleep(0.5)
renameNetwork("DisGeNET network")
createNodeSource("fromDisGeNET")
Sys.sleep(0.5)
sparqlquery("wp","nodequery.txt","wp_nodelist")
#Making a SPARQL query to the endpoint to get all nodes associated with a list of pathways
if (any(grepl("identifiers\\.org", wp_nodelist$Identifier))) {
# Checking whether the 'Identifier' column contains the identifiers.org URL
#This is to avoid issues later when the identifiers.org part is removed and the code is reran
wp_nodelist$WPNodeIDType <- gsub(".*/([^/]+)/.*", "\\1", wp_nodelist$Identifier)
#If 'identifiers.org' is still in the column, extract part of the string into a new column to see what type the identifier is
} else {
# If 'identifiers.org' is not found, do nothing
}
wp_nodelist[] <- lapply(wp_nodelist, function(x) str_replace_all(x, "https://identifiers\\.org/([^/]+)/", ""))
#Selecting and removing "https://identifiers.org/xyz" from every row in the df for improved readability
wp_nodelist$CNVassociated <- ifelse(grepl("copy number | CNV | deletion",wp_nodelist$PathwayTitle), 1, NA)
#Adding a new binary column showing if a given node is associated with a CNV based on pathway title
wp_nodelist$WPNodeID <- wp_nodelist$Identifier
#Generating a duplicate node identifier column since the original column will be lost during Cytoscape import due to it being selected as source column
write.table(wp_nodelist, file=paste0(getwd(),"/Data/WikiPathways/nodelist.tsv"), quote=FALSE, sep="\t", row.names=FALSE)
#Writing the modified file for Cytoscape import
commandsRun(sprintf('network import file columnTypeList="sa,sa,s,sa,sa,sa,sa,sa" file=%s firstRowAsColumnNames=true rootNetworkList=-- Create new network collection -- startLoadRow=1 delimiters=\\t', paste0(getwd(),"/Data/WikiPathways/nodelist.tsv")))
#Importing a list of nodes from the output of a WikiPathways SPARQL query (get all nodes in pathways matching the keyword 'Schizophrenia' and some manually selected pathways)
Sys.sleep(0.5)
#Adding sys.sleep to give Cytoscape sufficient time to import the file as network; otherwise, renaming doesn't always work since no network is selected until the import is complete
renameNetwork("WikiPathways nodes")
sparqlquery("wp","edgequery.txt","wp_edgelist")
#Making a SPARQL query to the endpoint to get a list of source-target pairs from selected pathways
wp_edgelist[] <- lapply(wp_edgelist, function(x) str_replace_all(x, "https://identifiers\\.org/([^/]+)/", ""))
#Selecting and removing "https://identifiers.org/xyz" from every row in the df for improved readability
edge_df <- wp_edgelist[grepl("Interaction",wp_edgelist$source) | grepl("Interaction",wp_edgelist$target),]
#Extracting rows containing "Interaction" in either the source or target column
#Interaction nodes represent phosphorylation and the like and are not suitable for the network
#They can still provide information about the connection of gene or other nodes so they can't just be deleted either
#If an Interaction node is connected to two or more non-interaction nodes, these nodes should be connected to each other, and the interaction node can be deleted
interaction_freq <- table(edge_df$target)
edge_df_filtered <- edge_df[edge_df$target %in% names(interaction_freq[interaction_freq > 1]),]
#Counting if a certain interaction occurs more than once; this implies that it is connected to more than one non-interaction node
unique_targets <- unique(edge_df_filtered$target)
for (target_val in unique_targets) {
# Identify rows with duplicate target values
rows_with_duplicate_target <- which(edge_df_filtered$target == target_val)
if (length(rows_with_duplicate_target) > 1) {
# Select one of the source values
source_val_to_transpose <- edge_df_filtered$source[rows_with_duplicate_target[1]]
# Transpose the source value to the target column in the row of the remaining source value
edge_df_filtered$target[rows_with_duplicate_target[-1]] <- source_val_to_transpose
# Remove duplicate rows
edge_df_filtered <- edge_df_filtered[-rows_with_duplicate_target[1], ]
}
}
#Transposing the non-identifier nodes for source-target pairs; if two nodes are associated with the same interaction, they become source-target pairs
wp_edgelist <- wp_edgelist <- wp_edgelist[!grepl(".*interaction.*", wp_edgelist$source, ignore.case = TRUE) &
!grepl(".*interaction.*", wp_edgelist$target, ignore.case = TRUE), ]
#Removing any row containing "Interaction"
wp_edgelist <- rbind(wp_edgelist,edge_df_filtered)
#Appending the new source-target pairs to the original edge list
write.table(wp_edgelist, file=paste0(getwd(),"/Data/WikiPathways/edgelist.tsv"), quote=FALSE, sep="\t", row.names=FALSE)
#Writing the modified file for Cytoscape import
commandsRun(sprintf('network import file columnTypeList="sa,s,t" file=%s firstRowAsColumnNames=true rootNetworkList=-- Create new network collection -- startLoadRow=1 delimiters=\\t', paste0(getwd(),"/Data/WikiPathways/edgelist.tsv")))
#Importing a list of source-target pairs from selected pathways from the ouput of a WikiPathways SPARQL query
Sys.sleep(0.5)
#Adding sys.sleep to give Cytoscape sufficient time to import the file as network; otherwise, renaming doesn't always work since no network is selected until the import is complete
renameNetwork("WikiPathways edges")
altmergeNetworks(sources = c("WikiPathways nodes","WikiPathways edges"),
title = "WikiPathways networks",
operation = "union",
nodeKeys=c("WPNodeID","name"))
#Union merging the node and edge networks to extend the node list with corresponding edges
Sys.sleep(0.5)
createNodeSource("fromWikiPathways")
deleteNetwork('WikiPathways nodes')
deleteNetwork('WikiPathways edges')
Sys.sleep(1)
#Pausing the script for 1 second - when letting the script run without this, the publication source creation fails
commandsRun(sprintf("network import file columnTypeList='sa,sa,source,sa,sa,sa,sa' file=%s firstRowAsColumnNames=true rootNetworkList=-- Create new network collection -- startLoadRow=1", paste0(getwd(),"/Data/Publications/Trubetskoy.txt")))
#Importing network from file
#List of 120 genes implicated in Trubetskoy et al., doi: 10.1038/s41586-022-04434-5
commandsRun("table rename column columnName=Ensembl.ID newColumnName=Ensembl table=Trubetskoy.txt default node")
#Renaming the Ensembl.ID column from the dataset to Ensembl for coherence with networks from other sources
commandsRun("table rename column columnName=Index.SNP newColumnName=snpID table=Trubetskoy.txt default node")
createNodeSource("fromPublication","10.1038/s41586-022-04434-5")
#Adding literature as  source to all imported nodes and adding the doi of the corresponding paper
renameNetwork("Trubetskoy risk genes")
#Renaming the newly imported network
metadata.add("Publications")
metadata.add("Trubetskoy et al. doi: 10.1038/s41586-022-04434-5")
metadata.add("")
networklist <- getNetworkList()
setCurrentNetwork(networklist[[1]])
for(i in 1:length(networklist)) {
current <- getNetworkName()
altmergeNetworks(c(current,networklist[[i]]), paste(current,networklist[[i]]),"union",inNetworkMerge = TRUE,nodeKeys=c("Ensembl","Ensembl"))
}
#Looping through the network list to merge all currently open networks with each other, creating one large unified network
renameNetwork("Schizophrenia supernetwork")
networklist <- getNetworkList()
snw_scz <- getNetworkName()
#Getting the name of the unified network to preserve it from deletion
lapply(networklist[networklist != snw_scz],deleteNetwork)
#Deleting all networks besides newly generated unified network
snw_ensembl <- getTableColumns("node","Ensembl")
#Getting all values in the Ensembl column of the supernetwork
input <- data.frame(
source = rep("En", length(snw_ensembl[, 1])),
identifier = snw_ensembl[, 1]
)
#Making a new df to be used as input for bridgedb
#Map Ensembl ID
snw_map <- maps(mapper,input,"H")
#Mapping from Ensembl to HGNC
snw_map <- select(snw_map, c("identifier", "mapping"))
snw_map <- rename(snw_map,
Ensembl = identifier,
HGNCsymbol = mapping)
#Selecting and renaming relevant columns from bridgeDb mapping output
loadTableData(snw_map,
data.key.column = "Ensembl",
table = "node",
table.key.column = "Ensembl")
#loading HGNC names for Ensembl IDs in supernetwork back to node table
exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW"),"CX", network = snw_scz, overwriteFile=TRUE)
#Exporting the supernetwork as cx file
end_section("Importing and merging")
## STRING --------------------------------------------------------------------------------------------------------------------------
start_section("STRING")
commandsRun('string stringify colDisplayName=name column=Ensembl compoundQuery=true cutoff=0.9 includeNotMapped=true  networkType="full STRING network" species="Homo sapiens" networkNoGui=current')
#Adding protein-protein interactions from STRING to the supernetwork
#Interactions between nodes are poorly preserved during importing and merging, and really only WikiPathways provides edge information while DisGeNET and the publication only provide gene lists
marked_cols <- as.list(getTableColumnNames()[!(getTableColumnNames() %in% c("selected","name.copy" ,"SUID","shared name","name","fromDisGeNET","fromWikiPathways","Ensembl","fromPublication","Publication.doi","CNVassociated","PathwayID","WPNodeID","WPNodeIDType","snpID","HGNCsymbol","DisGeNETname","disgenet_curated","gdascore","Entrez_gene"))])
lapply(marked_cols, function(column) {
deleteTableColumn(column=column)
})
#Filtering columns
renameNetwork("SCZ_SNW_STRING")
scz_snw_string <- getNetworkName()
exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW_STRING"),"CX",network=scz_snw_string,overwriteFile=TRUE)
#Exporting the filtered, stringified supernetwork as cx file and tagging it with the time and data to match with the metadata file
end_section("STRING")
## CLUSTERING ----------------------------------------------------------------------------------------------------------------------
start_section("Clustering")
createColumnFilter(filter.name="delete.noensembl", column="Ensembl","ENSG","DOES_NOT_CONTAIN")
deleteSelectedNodes()
metadata.add("GLay Clustering")
metadata.add(capture.output(commandsRun('cluster glay clusterAttribute=__glayCluster createGroups=false network=current restoreEdges=true showUI=true undirectedEdges=true')))
#Clustering the network using the GLay community cluster from the clusterMaker Cytoscape app and recording outcome to metadata
renameNetwork("SCZ_SNW_STRING_clustered")
renameTableColumn('__glayCluster','gLayCluster')
#Renaming the newly generated gLayCluster column as the original name with two underscores is not recognized during gene ontology
snw_scz_string_clustered <- getNetworkName()
clustered_nodetable <- paste0(other_savepath,sprintf("/%s node table.csv",snw_scz_string_clustered))
#Saving the file path to the node table for easier reading (note the double space between node and table)
commandsRun(sprintf('table export options=CSV outputFile=%1$s table="%2$s default  node"',clustered_nodetable,snw_scz_string_clustered))
#Exporting the node table as .csv file to the current session's "Network" folder
exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW_STRING_clustered"),"CX",network=snw_scz_string_clustered,overwriteFile=TRUE)
#Exporting the filtered, stringified, clustered supernetwork as cx file and tagging it with the time and data to match with the metadata file
read_clustered_nodetable <- read.csv(clustered_nodetable)
#Reading the exported csv
split_df <- split(read_clustered_nodetable$Ensembl,read_clustered_nodetable$gLayCluster)
#Splitting the node table by cluster
nodecount <- sapply(split_df, length)
#Counting how many nodes are in each cluster
countmatrix <- matrix(seq(1,length(nodecount)), ncol=1)
countmatrix <- cbind(countmatrix,as.numeric(nodecount))
#Construcing a matrix showing how many nodes are in each cluster
invalidclusters <- as.list(countmatrix[countmatrix[, 2] < 5, 1])
#Getting which clusters have fewer than 5 nodes associated with them
valid_clustered_nodetable <- read_clustered_nodetable[!read_clustered_nodetable$gLayCluster %in% invalidclusters, ]
#Generating a new df containing only nodes associated with clusters that had 5 or more nodes
split_tbl <- split(valid_clustered_nodetable, valid_clustered_nodetable$gLayCluster)
# test <- split_tbl[[1]]
# test2 <- test[, c("fromWikiPathways", "fromDisGeNET", "fromPublication")]
# colnames(test2) <- c("fromWikiPathways", "fromDisGeNET", "fromPublication")
#
# column_combinations_3 <- combn(colnames(test2),3,FUN=function(x) paste(x, collapse="_and_"))
# column_combinations_2 <- combn(colnames(test2),2,FUN=function(x) paste(x, collapse="_and_"))
# for (combination in column_combinations_2) {
#   test2[paste(combination,collapse="_and_")] <- test2[[combination[1]]] & test2[[combination[2]]]
# }
# for (combination in column_combinations_3) {
#   test2[paste(combination, collapse = "_")] <- test2[[combination[1]]] & test2[[combination[2]]] & test2[[combination[3]]]
# }
#
sourcecount <- function(cluster) {
wpcount <- sum(split_tbl[[cluster]][["fromWikiPathways"]] == 1, na.rm = TRUE)
dgcount <- sum(split_tbl[[cluster]][["fromDisGeNET"]] == 1, na.rm = TRUE)
litcount <- sum(split_tbl[[cluster]][["fromPublication"]] == 1, na.rm = TRUE)
result_df <- data.frame(
gLayCluster = split_tbl[[cluster]][["gLayCluster"]][1],
WikiPathways_source = wpcount,
DisGeNET_source = dgcount,
Publication_source = litcount
)
}
sources_count <- do.call(rbind, lapply(seq_along(split_tbl),sourcecount))
#For each cluster, counting how many nodes are associated with which sources
cnvassociatedcount <- function(cluster) {
cnvcount <- sum(split_tbl[[cluster]][["CNVassociated"]] == 1,na.rm=TRUE)
total_wp_nodes <- sources_count[sources_count$gLayCluster == split_tbl[[cluster]][["gLayCluster"]][1], "WikiPathways_source"]
non_cnv_count <- total_wp_nodes - cnvcount
result_df <- data.frame(
gLayCluster = split_tbl[[cluster]][["gLayCluster"]][1],
WikiPathways_CNV = cnvcount,
WikiPathways_noCNV = non_cnv_count
)
}
cnvassociated_count <- do.call(rbind,lapply(seq_along(split_tbl),cnvassociatedcount))
#For each cluster, count how many nodes originally come from CNV-associated pathways which pathways they come from
end_section("Clustering")
## GO ANALYSIS ------------------------------------------------------------------------------------------------------------------------
start_section("GO Analysis")
split_df <- split(valid_clustered_nodetable$Ensembl,valid_clustered_nodetable$gLayCluster)
split_list <- lapply(split_df, as.vector)
#Splitting the node table by cluster number, i.e. lists of Ensembl IDs are created per cluster
go <- function(cluster) {
gost(
query = cluster,
organism = "hsapiens",
ordered_query = FALSE,
multi_query = TRUE,
significant = TRUE,
exclude_iea = FALSE,
measure_underrepresentation = FALSE,
evcodes = FALSE,
user_threshold = 0.05,
correction_method = "g_SCS",
domain_scope ="annotated",
custom_bg = NULL,
numeric_ns = "",
sources = NULL,
as_short_link = FALSE,
highlight = TRUE
)
}
go_list <- lapply(split_list,go)
#Iterating the gost GO function over all clusters
saveRDS(go_list, file=paste0(other_savepath,"/go_list.rds"))
#Saving the entire generated GO analysis as R object locally
get_top_terms <- function(cluster) {
terms <- toString(go_list[[cluster]][["result"]][["term_name"]][1:5])
#Extracting the top 5 term names associated with each cluster
pval <- toString(go_list[[cluster]][["result"]][["p_values"]][1:5])
#Extracting the p-values for the corresponding top 5 term names
nodes <- paste(go_list[[cluster]][["meta"]][["query_metadata"]][["queries"]][["query_1"]],collapse=",")
nnodes <- str_count(toString(go_list[[cluster]][["meta"]][["query_metadata"]][["queries"]][["query_1"]]),"\\S+")
#Extracing the number of nodes/genes contained in each cluster
result_df <- data.frame(
gLayCluster = cluster,
GO_Terms = terms,
GO_Pvals = pval,
Nodes = nodes,
N_nodes = nnodes
)
}
topterms_df <- do.call(rbind, lapply(names(go_list),get_top_terms))
#Getting top 5 term names and corresponding p-values for each cluster and storing in topterms_df
topterms_df <- cbind(topterms_df,sources_count,cnvassociated_count)
#joining the cluster table and the table detailing the amount of sources per cluster
write.table(topterms_df, file=paste0(getwd(),"/Data/GO-clusters-vis.tsv"), sep = "\t",row.names=FALSE,quote=FALSE)
#Writing the table to file for Cytoscape import during visualisation
loadTableData(topterms_df,data.key.column="gLayCluster",table.key.column="gLayCluster")
#Loading the generated top terms and p-values back to the supernetwork; every gene belonging to cluster x is now associated with the top terms of cluster x
deleteTableColumn('gLayCluster.1')
#Deleting duplicate gLayCluster column that appears after importing top terms data back to network
renameNetwork(title=paste0(getNetworkName(),"_GO"))
snw_scz_filtered_string_clustered_go <- getNetworkName()
#Renaming and saving the network name to indicate addition of GO information
compare_term_id_lists <- function(list1, list2) {
common_elements <- intersect(list1, list2)
return(length(common_elements))
}
#Setting up a function to get intersections between cluster term IDs
match_df <- data.frame(Cluster1 = character(),
Cluster2 = character(),
Matches = numeric(),
stringsAsFactors = FALSE)
#Setting up a df to store output in
for (i in 1:(length(go_list) - 1)) {
for (j in (i + 1):length(go_list)) {
term_id_i <- go_list[[i]][["result"]][["term_id"]]
term_id_j <- go_list[[j]][["result"]][["term_id"]]
matches <- compare_term_id_lists(term_id_i, term_id_j)
match_df <- rbind(match_df, data.frame(Cluster1 = names(go_list)[i],
Cluster2 = names(go_list)[j],
Matches = matches))
#Iterating over go_list to compare GO term IDs between every cluster and store number of overlaps
}
}
colnames(match_df) <- c("source","target","GO_term_matches")
#Renaming columns
write.table(match_df, file=paste0(getwd(),"/Data/match_df.tsv"), sep = "\t",row.names=FALSE,quote=FALSE)
exportNetwork(filename=paste0(nw_savepath,"SCZ_SNW_STRING_clustered_GO"),"CX",network=snw_scz_filtered_string_clustered_go,overwriteFile=TRUE)
#Exporting the filtered, stringified, clustered supernetwork after GO as cx file and tagging it with the time and data to match with the metadata file
end_section("GO Analysis")
