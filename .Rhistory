networkname <- getNetworkName()
nodetable <- paste0(networkname," default node")
}
commandsRun(sprintf("table create column columnName=fromWikiPathways table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromDisGeNET table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromPublication table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=Publication.doi table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromSTRING table=%s type=string",nodetable))
#Creating a new column for each source used for all networks
if ( source == "STRINGnode") {
commandsRun(sprintf('table set values columnName=%1$s handleEquations=false rowList="selected:true" table=%2$s value=1',source,nodetable))
}
else {
commandsRun(sprintf("table set values columnName=%1$s handleEquations=false rowList=all table=%2$s value=1",source,nodetable))
#Filling the new column of the corresponding source with 1 to indicate which source the node is imported from
}
if (!is.null(doi)) {
commandsRun(sprintf("table set values columnName=Publication.doi handleEquations=false rowList=all table=%1$s value=%2$s",nodetable,doi))
#Adding doi for literature used if provided
}
}
#Function to create new column in node table specifying origin of network/node
disgenetRestUrl<-function(netType,host="127.0.0.1",port=1234,version="v7"){
if(is.null(netType)){
print("Network type not specified.")
}else{
disgeneturl<-sprintf("http://%s:%i/disgenet/%s/%s",host,port,version,netType)
}
return (disgeneturl)
}
net <- "gene-disease-net"
disgenetRestUrl(netType = net)
#Defining object for REST to call DisGeNET automation module; defining that we will be using gene-disease associations (GDA)
disgenetRestCall<-function(netType,netParams){
disgeneturl<-disgenetRestUrl(netType)
restCall<-POST(disgeneturl, body = netParams, encode = "json")
result<-content(restCall,"parsed")
return(result)
}
#Object that executes REST calls to DisGeNET module in Cytoscape
geneDisParams <- function(source,dis,min) {list(
source = source,
assocType = "Any",
diseaseClass = "Any",
diseaseSearch = dis,
geneSearch = " ",
initialScoreValue = min,
finalScoreValue = "1.0"
)}
#Specifying parameters of the GDA network to be imported
#queryspecies.wp <- c("Homo sapiens","Rattus norvegicus","Mus musculus")
#getPathways.wp <- function(i) {
#pw <- findPathwaysByText(i)
#pw <- pw %>%
#dplyr::filter(species %in% queryspecies.wp)
#Filtering by species
# pw.ids <- paste0(i, "_wpids")
# assign(pw.ids, as.character(pw$id),envir = .GlobalEnv)
#Extracting WP IDs
#}
#Function to query WikiPathways using keyword and to extract WP IDs for the import function
#import <- function(j) {
#commandsRun(paste0('wikipathways import-as-network id=', j))
#Pasting WikiPathways IDs into a Cytoscape command line prompt to import as networks
#createNodeSource("WikiPathways")
#Filling the 'WikiPathways' column with 1 to indicate the source
#}
#Importing pathways from WikiPathways by pathway ID
# SCHIZOPHRENIA =======================================================================================================================
## IMPORTING AND MERGING ---------------------------------------------------------------------------------------------------------------
sparqlquery("metadataquery.txt","WikiPathways-SPARQL-metadata")
#Getting the metadata of the endpoint used for the WikiPathways SPARQL queries
metadata.add(`WikiPathways-SPARQL-metadata`)
#Adding the fetched metadata to the metadata file for the session
#It is technically possible that the metadata would describe an earlier version of the RDF if it is updated while the script runs but this is unlikely
sparqlquery("pathwayquery.txt","wp_pathwaylist")
sparqlquery <- function(queryfile,output) {
file_path <- paste0(getwd(),sprintf("/Data/WikiPathways/%s",queryfile))
#Specifying file path to the .txt file containing the query
querybody <- paste(readLines(file_path), collapse = "")
#Reading a text file containing a SPARQL query stored in the repo
encoded_query <- URLencode(querybody)
#Encoding the query as URL
base_url <- "https://sparql.wikipathways.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
full_url <- paste0(base_url, encoded_query, "&format=text/html&timeout=0&signal_void=on")
#Joining the query from the text file and the base URL and adding that output is desired as HTML
html <- read_html(full_url)
#Sending the query to the SPARQL endpoint and extracting as HTML
tablebody <- html %>%
html_element("body") %>%
html_element("div") %>%
html_element("table")
#Navigating to the table output by the SPARQL query
assign(output, html_table(tablebody), envir=.GlobalEnv)
#Getting the output as tibble
}
#Adding the fetched metadata to the metadata file for the session
#It is technically possible that the metadata would describe an earlier version of the RDF if it is updated while the script runs but this is unlikely
sparqlquery("pathwayquery.txt","wp_pathwaylist")
setwd("~/GitHub/SCZ-CNV")
#Setting working directory
rm(list=ls())
#Cleaning up workspace
packages <- c("dplyr","httr","stringr","gprofiler2","rvest")
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
if(!"rWikiPathways" %in% installed.packages()){
if (!requireNamespace("BiocManager", quietly=TRUE))
install.packages("BiocManager")
BiocManager::install("rWikiPathways")
}
if(!"RCy3" %in% installed.packages()){
if (!requireNamespace("BiocManager", quietly=TRUE))
install.packages("BiocManager")
BiocManager::install("RCy3")
}
#Checking if required packages are installed and installing if not
#Different structure for rWikiPathways and RCy3 packages as these are not installed directly but via the BiocManager package
invisible(lapply(c(packages,"rWikiPathways","RCy3"), require, character.only = TRUE))
#Loading libraries
sysdatetime <- Sys.time()
datetime <- format(sysdatetime, format = "%Y-%m-%d_%Hh%M")
dir.create("Outputs")
dir.create(sprintf("Outputs/Session-%s",datetime))
dir.create(sprintf("Outputs/Session-%s/Networks",datetime))
#Creating directories for outputs generated by this script to be saved in; new "Session" folder created each time the script is ran (contains generated networks, metadata, and sessionInfo)
nw_savepath <- sprintf("%1$s/Outputs/Session-%2$s/Networks/",getwd(),datetime)
file.create(sprintf("Outputs/Session-%s/metadata.txt",datetime))
#Creating a new metadata file with the current date and time as suffix for easier organisation
#Such a metadata file should be generated every time this script is ran to record parameters and versions of functions or databases, including the time avoids files being overwritten if the script is run multiple times a day (can even include seconds if script is ran multiple times per minute)
metadata.add <- function(info) {
write(sapply(info, as.character), sprintf("Outputs/Session-%s/metadata.txt",datetime),append=TRUE, sep = "\n")
}
metadata.add(sysdatetime)
metadata.add(Sys.timezone())
metadata.add("")
#Adding the timezone, date, and time to the metadata
invisible(file.create(sprintf("Outputs/Session-%s/sessioninfo-%s.txt",datetime,datetime)))
writeLines(capture.output(sessionInfo()),sprintf("Outputs/Session-%s/sessioninfo-%s.txt",datetime,datetime))
#Generating and adding a sessionInfo file to the current session output folder
cytoscapePing()
cytoscapeVersionInfo()
#Checking if Cytoscape is running and version info
metadata.add(capture.output(cytoscapeVersionInfo()))
checkinstall.app <- function(app) {
status_string <- getAppStatus(app)
#Getting install status of app
words <- strsplit(status_string, " ")[[1]]
last_word <- tail(words, 1)
#getAppStatus returns a character string instead of a logical value, so the last word (usually either "Installed" or "Uninstalled") from the output is checked
if (last_word == "Installed") {
print(sprintf("App %s is already installed.",app))
} else {
installApp(app)
print(sprintf("Installed app %s.",app))
}
}
#Function to check whether required Cytoscape apps are installed and installing them if not
applist <- c("Wikipathways", "DisGeNET-app", "CyTargetLinker","stringApp","BridgeDb","clusterMaker2")
#WikiPathways v.3.3.10
#DisGeNET-app v.7.3.0
#CyTargetLinker v. 4.1.0
#stringApp v. 2.0.2
#BridgeDb v.1.2.0
#clusterMaker2 v.2.3.4
lapply(applist,checkinstall.app)
#Checking and installing (if required) necessary Cytoscape apps
lapply(applist,getAppInformation)
metadata.add("Required Cytoscape apps and versions:")
invisible(metadata.add(print(lapply(applist,getAppInformation))))
metadata.add("")
# FUNCTION DICTIONARY-------------------------------------------------------------------------------------------------------------------
.defaultBaseUrl <- 'http://127.0.0.1:1234/v1'
#Defining the default base URL found in the RCy3 source as R object for altmergeNetworks
altmergeNetworks <- function(               sources = NULL,
title = NULL,
operation = "union",
nodeKeys = NULL,
nodeMergeMap = NULL,
nodesOnly = FALSE,
edgeKeys = NULL,
edgeMergeMap = NULL,
networkMergeMap = NULL,
inNetworkMerge = TRUE,
base.url = .defaultBaseUrl) {
cmd.string <- 'network merge' # a good start
# sources must be suppled
if(is.null(sources)) {
message("Missing sources!")
return(NULL)
} else {
sources.str <- paste(sources, collapse = ",")
cmd.string <- paste0(cmd.string,' sources="',sources.str,'"')
}
# defaults
cmd.string <- paste0(cmd.string,' operation=',operation)
cmd.string <- paste0(cmd.string,' nodesOnly=',nodesOnly)
cmd.string <- paste0(cmd.string,' inNetworkMerge=',inNetworkMerge)
# optional args
if(!is.null(title))
cmd.string <- paste0(cmd.string,' netName="',title,'"')
if(!is.null(nodeKeys))
cmd.string <- paste0(cmd.string,' nodeKeys="',paste(nodeKeys, collapse = ","),'"')
if(!is.null(edgeKeys))
cmd.string <- paste0(cmd.string,' edgeKeys="',paste(edgeKeys, collapse = ","),'"')
if(!is.null(nodeMergeMap)){
nodeMergeMap.str <- paste(nodeMergeMap, collapse = ",")
nodeMergeMap.str <- gsub("c\\(", "{", nodeMergeMap.str)
nodeMergeMap.str <- gsub("\\)", "}", nodeMergeMap.str)
cmd.string <- paste0(cmd.string,' nodeMergeMap="',nodeMergeMap.str,'"')
}
if(!is.null(edgeMergeMap)){
edgeMergeMap.str <- paste(edgeMergeMap, collapse = ",")
edgeMergeMap.str <- gsub("c\\(", "{", edgeMergeMap.str)
edgeMergeMap.str <- gsub("\\)", "}", edgeMergeMap.str)
cmd.string <- paste0(cmd.string,' edgeMergeMap="',edgeMergeMap.str,'"')
}
if(!is.null(networkMergeMap)){
networkMergeMap.str <- paste(networkMergeMap, collapse = ",")
networkMergeMap.str <- gsub("c\\(", "{", networkMergeMap.str)
networkMergeMap.str <- gsub("\\)", "}", networkMergeMap.str)
cmd.string <- paste0(cmd.string,' networkMergeMap="',networkMergeMap.str,'"')
}
res.data <- commandsPOST(cmd.string, base.url = base.url)
if(!is.null(res.data$SUID))
return(res.data$SUID)
else
return(res.data)
}
#Normally, RCy3's 'mergeNetworks' function would be used to unify imported networks into one supernetwork
#This function does however not work on the latest RCy3 release (v.2.22.1), but does work when running the script on RCy3 v.2.14.2
#RCy3 2.14.2 requires R v.4.1.3, requiring the entire script to run on an old version of R for one function that is used once
#Here, we redefine the function using the source code from RCy3 v.2.14.2 and simply use this alternate function to merge networks
sparqlquery <- function(queryfile,output) {
file_path <- paste0(getwd(),sprintf("/Data/WikiPathways/%s",queryfile))
#Specifying file path to the .txt file containing the query
querybody <- paste(readLines(file_path), collapse = "")
#Reading a text file containing a SPARQL query stored in the repo
encoded_query <- URLencode(querybody)
#Encoding the query as URL
base_url <- "https://sparql.wikipathways.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
full_url <- paste0(base_url, encoded_query, "&format=text/html&timeout=0&signal_void=on")
#Joining the query from the text file and the base URL and adding that output is desired as HTML
html <- read_html(full_url)
#Sending the query to the SPARQL endpoint and extracting as HTML
tablebody <- html %>%
html_element("body") %>%
html_element("div") %>%
html_element("table")
#Navigating to the table output by the SPARQL query
assign(output, html_table(tablebody), envir=.GlobalEnv)
#Getting the output as tibble
}
#Function to send a SPARQL query defined in a local text file to the endpoint and extract to desired dataframe
createNodeSource <- function(source,doi=NULL) {
if (source == "WikiPathways") {
networkname <- getNetworkName()
nodetable <- paste0(networkname," default node")
}
#Networks imported from WikiPathways have a type in the node table designations, as they have 2 spaces between "default" and "node" instead of one
#This check determines which node table name format is to be applied depending on the source (WikiPathways or other)
else {
networkname <- getNetworkName()
nodetable <- paste0(networkname," default node")
}
commandsRun(sprintf("table create column columnName=fromWikiPathways table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromDisGeNET table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromPublication table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=Publication.doi table=%s type=string",nodetable))
commandsRun(sprintf("table create column columnName=fromSTRING table=%s type=string",nodetable))
#Creating a new column for each source used for all networks
if ( source == "STRINGnode") {
commandsRun(sprintf('table set values columnName=%1$s handleEquations=false rowList="selected:true" table=%2$s value=1',source,nodetable))
}
else {
commandsRun(sprintf("table set values columnName=%1$s handleEquations=false rowList=all table=%2$s value=1",source,nodetable))
#Filling the new column of the corresponding source with 1 to indicate which source the node is imported from
}
if (!is.null(doi)) {
commandsRun(sprintf("table set values columnName=Publication.doi handleEquations=false rowList=all table=%1$s value=%2$s",nodetable,doi))
#Adding doi for literature used if provided
}
}
#Function to create new column in node table specifying origin of network/node
disgenetRestUrl<-function(netType,host="127.0.0.1",port=1234,version="v7"){
if(is.null(netType)){
print("Network type not specified.")
}else{
disgeneturl<-sprintf("http://%s:%i/disgenet/%s/%s",host,port,version,netType)
}
return (disgeneturl)
}
net <- "gene-disease-net"
disgenetRestUrl(netType = net)
#Defining object for REST to call DisGeNET automation module; defining that we will be using gene-disease associations (GDA)
disgenetRestCall<-function(netType,netParams){
disgeneturl<-disgenetRestUrl(netType)
restCall<-POST(disgeneturl, body = netParams, encode = "json")
result<-content(restCall,"parsed")
return(result)
}
#Object that executes REST calls to DisGeNET module in Cytoscape
geneDisParams <- function(source,dis,min) {list(
source = source,
assocType = "Any",
diseaseClass = "Any",
diseaseSearch = dis,
geneSearch = " ",
initialScoreValue = min,
finalScoreValue = "1.0"
)}
#Specifying parameters of the GDA network to be imported
#queryspecies.wp <- c("Homo sapiens","Rattus norvegicus","Mus musculus")
#getPathways.wp <- function(i) {
#pw <- findPathwaysByText(i)
#pw <- pw %>%
#dplyr::filter(species %in% queryspecies.wp)
#Filtering by species
# pw.ids <- paste0(i, "_wpids")
# assign(pw.ids, as.character(pw$id),envir = .GlobalEnv)
#Extracting WP IDs
#}
#Function to query WikiPathways using keyword and to extract WP IDs for the import function
#import <- function(j) {
#commandsRun(paste0('wikipathways import-as-network id=', j))
#Pasting WikiPathways IDs into a Cytoscape command line prompt to import as networks
#createNodeSource("WikiPathways")
#Filling the 'WikiPathways' column with 1 to indicate the source
#}
#Importing pathways from WikiPathways by pathway ID
#Adding the fetched metadata to the metadata file for the session
#It is technically possible that the metadata would describe an earlier version of the RDF if it is updated while the script runs but this is unlikely
sparqlquery("pathwayquery.txt","wp_pathwaylist")
#Adding the fetched metadata to the metadata file for the session
#It is technically possible that the metadata would describe an earlier version of the RDF if it is updated while the script runs but this is unlikely
sparqlquery("pathwayquery.txt","wp_pathwaylist")
View(wp_pathwaylist)
writeLines(lines, con="Data/WikiPathways/automaticpathways.txt")
#Getting a list of pathways corresponding to a keyword as per defined in the query
lines <- apply(wp_pathwaylist$PWID, 1, function(row) paste(row), collapse=" ")
#Getting a list of pathways corresponding to a keyword as per defined in the query
lines <- apply(wp_pathwaylist$PWID, 1, function(row) paste(row), collapse=" ")
#Getting a list of pathways corresponding to a keyword as per defined in the query
lines <- apply(as.character(wp_pathwaylist$PWID), 1, function(row) paste(row), collapse=" ")
#Getting a list of pathways corresponding to a keyword as per defined in the query
writeLines(wp_pathwaylist[["PWID"]], con="Data/WikiPathways/automaticpathways.txt")
#Saving the output pathway list to file
manualpathways <- readLines("Data/WikiPathways/manualpathways.txt")
#Saving the output pathway list to file
manualpathways <- readLines("Data/WikiPathways/manualpathways.txt")
sparqlquery("pathwayquery.txt","wp_pathwaylist")
View(wp_pathwaylist)
#Getting a list of pathways corresponding to a keyword as per defined in the query
writeLines(wp_pathwaylist, con="Data/WikiPathways/automaticpathways.txt")
allpathways <- c(wp_pathwaylist[["PWID"]],manualpathways)
View(allpathways)
allpathways
my_vector <- allpathways
if (any(duplicated(my_vector))) {
print("my_vector contains duplicates.")
} else {
print("my_vector does not contain duplicates.")
}
allpathways_URL <- paste0("<",allpathways,">")
#Adding <> around all entries for easier use in SPARQL queries
#URLs need to be surrounded by <> to be recognised as such
writeLines(allpathways_URL, con="Data/WikiPathways/Pathwaylists/allpathways.txt")
# SCHIZOPHRENIA =======================================================================================================================
## IMPORTING AND MERGING ---------------------------------------------------------------------------------------------------------------
sparqlquery("wp","metadataquery.txt","WikiPathways-SPARQL-metadata")
sparqlquery <- function(endpoint,queryfile,output) {
if (tolower(endpoint) %in% c("WikiPathways","wikipathways","wp","WP")) {
file_path <- paste0(getwd(),sprintf("/Data/WikiPathways/%s",queryfile))
#Specifying file path to the .txt file containing the query
base_url <- "https://sparql.wikipathways.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else if (tolower(endpoint) %in% c("AOP-Wiki", "aopwiki","aop-wiki","AOPWiki")) {
file_path <- paste0(getwd(),sprintf("/Data/AOP-Wiki/%s",queryfile))
#Specifying file path to the .txt file containing the query
base_url <- "https://aopwiki.rdf.bigcat-bioinformatics.org/sparql/?default-graph-uri=&query="
#Defining the base URL preceding all SPARQL URLs for the endpoint
} else {
print("Error: Please specify either WikiPathways or AOP-Wiki as endpoint.")
}
querybody <- paste(readLines(file_path), collapse = "")
#Reading a text file containing a SPARQL query stored in the repo
encoded_query <- URLencode(querybody)
#Encoding the query as URL
full_url <- paste0(base_url, encoded_query, "&format=text/html&timeout=0&signal_void=on")
#Joining the query from the text file and the base URL and adding that output is desired as HTML
html <- read_html(full_url)
#Sending the query to the SPARQL endpoint and extracting as HTML
tablebody <- html %>%
html_element("body") %>%
html_element("div") %>%
html_element("table")
#Navigating to the table output by the SPARQL query
assign(output, html_table(tablebody), envir=.GlobalEnv)
#Getting the output as tibble
}
# SCHIZOPHRENIA =======================================================================================================================
## IMPORTING AND MERGING ---------------------------------------------------------------------------------------------------------------
sparqlquery("wp","metadataquery.txt","WikiPathways-SPARQL-metadata")
View(`WikiPathways-SPARQL-metadata`)
sparqlquery("AOP-Wiki","KERKEquery.txt","KERList2")
View(KERList2)
KERList <-  read.table(file=paste0(getwd(),"/Data/AOP-Wiki/KERList.tsv"),header=TRUE, sep ="\t")
View(KERList)
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
KEEnsembl <- read.table(file=paste0(getwd(),"/Data/AOP-Wiki/KEEnsembl.tsv"),header=TRUE, sep ="\t")
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
sparqlquery("AOP-Wiki","KEensemblquery.txt","KEensembl2")
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
sparqlquery("AOP-Wiki","KEensemblquery.txt","KEensembl2")
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
sparqlquery("AOP-Wiki","KEensemblquery.txt","KEensembl2")
s
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
sparqlquery("AOP-Wiki","KEensemblquery.txt","KEensembl2")
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
sparqlquery("AOP-Wiki","KEensemblquery.txt","KEensembl2")
#Sending a query to AOP-Wiki to get all KE-Ensembl ID pairings available
allKEsFromList <- data.frame(allKEs = c(KERList$KEup,KERList$KEdown))
#Combining both up- and downregulated KEs into a single column
filtered_KEEnsembl <- subset(KEEnsembl, KE %in% allKEsFromList$allKEs)
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
sparqlquery("AOP-Wiki","KEensemblquery.txt","KEensembl")
#Sending a query to AOP-Wiki to get all KE-Ensembl ID pairings available
allKEsFromList <- data.frame(allKEs = c(KERList$KEup,KERList$KEdown))
#Combining both up- and downregulated KEs into a single column
filtered_KEEnsembl <- subset(KEEnsembl, KE %in% allKEsFromList$allKEs)
#Combining both up- and downregulated KEs into a single column
filtered_KEEnsembl <- subset(KEEnsembl, KE %in% allKEsFromList$allKEs)
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
sparqlquery("AOP-Wiki","KEensemblquery.txt","KEEnsembl")
#Combining both up- and downregulated KEs into a single column
filtered_KEEnsembl <- subset(KEEnsembl, KE %in% allKEsFromList$allKEs)
View(KERList)
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
write.table(KERList, file="/Data/AOP-Wiki/KERList.tsv",quote=FALSE,row.names=FALSE,sep="\t")
setwd("~/GitHub/SCZ-CNV")
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
write.table(KERList, file="/Data/AOP-Wiki/KERList.tsv",quote=FALSE,row.names=FALSE,sep="\t")
#Sending a query to AOP-Wiki as specified in the query file to get KERs and corresponding KEs for selected AOs
write.table(KERList, file=paste0(getwd(),"/Data/AOP-Wiki/KERList.tsv"),quote=FALSE,row.names=FALSE,sep="\t")
commandsRun(sprintf('network import file columnTypeList="ea,s,t" file=%s firstRowAsColumnNames=true startLoadRow=1 rootNetworkList=-- Create new network collection --',paste0(getwd(),"/Data/AOP-Wiki/KERList.tsv")))
commandsRun(sprintf('network import file columnTypeList="ea,s,t" file=%s firstRowAsColumnNames=true startLoadRow=1 rootNetworkList=-- Create new network collection --',paste0(getwd(),"/Data/AOP-Wiki/KERList.tsv")))
commandsRun(sprintf('network import file columnTypeList="ea,s,t" file=%s firstRowAsColumnNames=true startLoadRow=1 rootNetworkList=-- Create new network collection --',paste0(getwd(),"/Data/AOP-Wiki/KERList.tsv")))
#Loading selected KERs into Cytoscape as new network
commandsRun(sprintf('table import file dataTypeTargetForNetworkCollection="Node Table Columns" delimiters=\\t file=%s firstRowAsColumnNames=true keyColumnForMapping="shared name" keyColumnIndex=1 startLoadRow=1',paste0(getwd(),"/Data/AOP-Wiki/KEMap.tsv")))
sparqlquery("AOP-Wiki","KEtitlequery.txt","KEMap.tsv")
sparqlquery("AOP-Wiki","KEtitlequery.txt","KEMap")
write.table(KEMap,file=paste0(getwd(),"/Data/AOP-Wiki/KEMap.tsv"))
#Loading a KE mapping file generated from a second SPARQL query that fetches all KE URIs and their titles from AOPwiki into the KER network
#This file also had quotation marks removed using a text editor
commandsRun(sprintf('network import file columnTypeList="s,t" file=%s firstRowAsColumnNames=true startLoadRow=1 rootNetworkList=-- Create new network collection --',paste0(getwd(),"/Data/AOP-Wiki/KEEnsembl_filtered.tsv")))
#Loading the filtered KE-ENSG list as new network into Cytoscape
altmergeNetworks(sources = c('KERList.tsv','KEEnsembl_filtered.tsv'),
title='KERs',
operation='union'
)
#Merging the manually curated KERList network and the KE-ENSG list to extend the selected KEs with associated genes
deleteNetwork(network='KERList.tsv')
deleteNetwork(network='KEEnsembl_filtered.tsv')
#Deleting networks used to make merged network
mapTableColumn(
column = 'name',
species= 'Human',
map.from = 'Ensembl',
map.to = 'HGNC',
force.single = 'true'
)
renameTableColumn('HGNC','Name2')
commandsRun(sprintf('table export options=CSV outputFile=%s table="KERs default node"', paste0(getwd(),"/Data/AOP-Wiki/KE_table.csv")))
KE_table <- read.csv(paste0(getwd(),"/Data/AOP-Wiki/KE_table.csv"))
#Loading the previously exported node table as R object
KE_table <- KE_table %>%
mutate(Ensembl=ifelse(grepl("ENSG",name),name,NA))
#Getting and transposing ENSG IDs from the name col to a new 'Ensembl' col to be used as key
KE_table <- KE_table %>%
mutate(KE_URI=ifelse(!grepl("ENSG",name),name,NA))
#Getting and transposing KE URIs (all rows not containing 'ENSG') from the name col to a new 'KE_URI' col
#These two steps are done as the 'name' and 'shared name' columns will be difficult to work with since they mix both ENSG and other data types
KE_table <- KE_table %>%
mutate(fromAOPwiki=1)
#Adding a new column to the node table indicating that the gene nodes in the KE network are imported from AOPwiki
KE_table <- KE_table %>%
select(-shared.name)
#Loading the node table in R changes the name the 'shared name' column to 'shared.name' to avoid a space, this results in a duplicate 'shared.name' column when importing the table back to Cytoscape
#For this reason, 'shared.name'/'shared name' is simply removed from the df as 'names' can also be used for mapping and contains the same information anyways
write.csv(KE_table, file=paste0(getwd(),"/Data/AOP-Wiki/KE_table.csv"), row.names=FALSE)
#Overwriting the previously exported node table with the updated version
loadTableData(
data = read.table(file=paste0(getwd(),"/Data/AOP-Wiki/KE_table.csv"),header=TRUE, sep =","),
data.key.column = "name",
table.key.column = "name"
)
#Loading the updated node table back to the network
altmergeNetworks(sources = c('SCZ_SNW_filtered_STRING_clustered_GO','KERs'),
title='SCZ_SNW_filtered_STRING_clustered_GO_KER',
operation='union',
nodeKeys = c('Ensembl','Ensembl')
)
#Merging the KER network with the supernetwork based on Ensembl ID
#As this is an union merge, all nodes from the KER network are introduced to the SNW, but the goal was simply to annotate existing genes in the SNW with KEs
#Therefore, some filtering is needed
sparqlquery("AOP-Wiki","AOPAOquery.txt","AOPs_AOs_for_selected_KEs")
write.table(AOPs_AOs_for_selected_KEs,file=paste0(getwd(),"/Data/AOP-Wiki/AOPs_AOs_for_selected_KEs.tsv"))
